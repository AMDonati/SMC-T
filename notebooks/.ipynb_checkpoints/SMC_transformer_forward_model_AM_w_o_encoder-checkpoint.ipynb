{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R2nCVX5Cb_g6"
   },
   "source": [
    "## Modified version of the Transformer with sequential Monte-Carlo Methods applied to Language Modelling\n",
    "* start from this notebook: https://www.tensorflow.org/tutorials/text/transformer\n",
    "* Implement the \"SMC Transformer forward Model\" presented here: https://drive.google.com/open?id=1ms8B9dYCU9yiMtkrEX2LzOEDD7EJJYQI \n",
    "* Use the dataset from this tutorial: https://www.tensorflow.org/tutorials/text/text_generation\n",
    "\n",
    "\n",
    "* Pre-trained models - Hugging Face: https://github.com/huggingface/transformers\n",
    "\n",
    "* See also this tutorial on Transfer Learning for NLP from NNACL 2019: https://colab.research.google.com/drive/1iDHCYIrWswIKp-n-pOg69xLoZO09MEgf\n",
    "\n",
    "* github version: https://github.com/huggingface/naacl_transfer_learning_tutorial\n",
    "\n",
    "### experiments\n",
    "#### PTB code: \n",
    "* https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py\n",
    "* https://github.com/sebastianruder/tensorflow-experiments/blob/master/rnn_ptb.py\n",
    "\n",
    "#### Wikipedia Dataset (included in tensorflow.dataset library). \n",
    "\n",
    "#### code of the Particle Filter Recurrent Neural Network \n",
    "* https://github.com/AdaCompNUS/pfnet\n",
    "\n",
    "#### About 'automatic' metrics to measure diversity/quality of language: see the link below\n",
    "* https://thegradient.pub/understanding-evaluation-metrics-for-language-models/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2-YwmQwt1A7"
   },
   "source": [
    "### to do (14/11/2019)\n",
    "\n",
    "#### Model part\n",
    "* Recode and debug pass_forward_from_layer class to have the option of 'decoder-level' SMC (what is done right now), and 'layer-level' SMC (OPTIONAL AND NOT URGENT)\n",
    "* Correct decoding with teacher forcing instead of with predictions\n",
    "* Code the option to choose between sampling method (what is done right now) and 'greedy' method (take previous as input to compute attention parameters). \n",
    "\n",
    "#### Inference part: \n",
    "* adapt the evaluate function with only the decoder and the right dataset. \n",
    "* code the function that predicts a new word Yk from a sequence of words Y0:k-1 (see language_models.pdf)\n",
    "* check if it works (do unit tests)\n",
    "\n",
    "#### training part\n",
    "* Implement the computation of the gradient\n",
    "* See what are the 'real' trainable parameters, and the ones for which no gradient computation is needed > use the function tf.stop_gradient (on the resampling weights & on the ancestral index matrix)\n",
    "* Implement the computation of the gradient as a custom loss? > cf this tuto: https://colab.research.google.com/drive/1blsyWmNzhSrvMsxIEp3g2-X-zk4FyFzx\n",
    "\n",
    "#### experiments\n",
    "* Change the prediction task to a word-level LM (instead of a character-level one)\n",
    "\n",
    "#### formating\n",
    "* Transform this code in a github repo with .py files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J0Qjg6vuaHNt"
   },
   "source": [
    "# 1. TRANSFORMER MODEL WITH PARTICLE FILTERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AOpGoE2T-YXS"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/transformer\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/transformer.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/transformer.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M-f8TnGpE_ex"
   },
   "source": [
    "The core idea behind the Transformer model is ** self-attention—the ability to attend to different positions of the input sequence to compute a representation of that sequence.** Transformer creates stacks of self-attention layers and is explained below in the sections *Scaled dot product attention* and *Multi-head attention*.\n",
    "\n",
    "A transformer model handles variable-sized input using stacks of self-attention layers instead of [RNNs](text_classification_rnn.ipynb) or [CNNs](../images/intro_to_cnns.ipynb). This general architecture has a number of advantages:\n",
    "\n",
    "* It make no assumptions about the temporal/spatial relationships across the data. This is ideal **for processing a set of objects** (for example, [StarCraft units](https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/#block-8)).\n",
    "* **Layer outputs can be calculated in parallel**, instead of a series like an RNN.\n",
    "* Distant items can affect each other's output without passing through many RNN-steps, or convolution layers (see [Scene Memory Transformer](https://arxiv.org/pdf/1903.03878.pdf) for example).\n",
    "* It can **learn long-range dependencies.** This is a challenge in many sequence tasks.\n",
    "\n",
    "The downsides of this architecture are:\n",
    "\n",
    "* For a time-series, the output for a time-step is calculated from the *entire history* instead of only the inputs and current hidden-state. This _may_ be less efficient.   \n",
    "* If the input *does* have a  temporal/spatial relationship, like text, **some positional encoding must be added or the model will effectively see a bag of words.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JjJJyJTZYebt"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fd1NWMxjfsDd"
   },
   "source": [
    "## Setup input pipeline\n",
    "* From this colab botebook: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb#scrollTo=EHDoRoc5PKWz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read the data\n",
    "\n",
    "First, look in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "outputs": [],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpIZvoeZfP5a"
   },
   "source": [
    "### Creating a word-level vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-R1fC_evfWF9"
   },
   "source": [
    "### Vectorizing the text at word-level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorize the text\n",
    "\n",
    "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "Now we have an integer representation for each character. Notice that we mapped the character as indexes from 0 to `len(unique)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FYyNlCNXymwY"
   },
   "outputs": [],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1VKcQHcymwb"
   },
   "outputs": [],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### The prediction task\n",
    "#### NB: change the task to a word-level prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "**Given a character, or a sequence of characters, what is the most probable next character? This is the task we're training the model to perform.**\n",
    "The input to the model will be a sequence of characters, and we train the model to predict the output—the following character at each time step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create training examples and targets\n",
    "\n",
    "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
    "\n",
    "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
    "\n",
    "**So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".**\n",
    "\n",
    "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "The `batch` method lets us easily convert these individual characters to sequences of the desired size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiCopyGZymwi"
   },
   "source": [
    "Print the first examples input and target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_33OHL3b84i0"
   },
   "source": [
    "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same thing but the `RNN` considers the previous step context in addition to the current input character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0eBu9WZG84i0"
   },
   "outputs": [],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches\n",
    "\n",
    "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nBQuibYA4n0n"
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "Since this model doesn't contain any recurrence or convolution, **positional encoding is added to give the model some information about the relative position of the words in the sentence.**\n",
    "\n",
    "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
    "\n",
    "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
    "\n",
    "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
    "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WhIOZjMNKujn"
   },
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Rz82wEs5biZ"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DWVLdmm1k2CN"
   },
   "outputs": [],
   "source": [
    "def positional_encoding_SMC(position, d_model, num_particles):\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "  pos_encoding=pos_encoding[:,np.newaxis,:,:]\n",
    "\n",
    "  pos_encoding=tf.tile(pos_encoding, [1,num_particles,1,1])\n",
    "  print(pos_encoding.shape)\n",
    "    \n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1kLCla68EloE"
   },
   "outputs": [],
   "source": [
    "pos_encoding = positional_encoding(50, 512)\n",
    "print (pos_encoding.shape)\n",
    "\n",
    "pos_encoding_SMC=positional_encoding_SMC(50,512,10)\n",
    "print('pos_encoding_SMC', positional_encoding_SMC)\n",
    "\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a_b4ou4TYqUN"
   },
   "source": [
    "## Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s42Uydjkv0hF"
   },
   "source": [
    "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2i8-e1s8ti9"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "  \n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7BYeBCNvi7n"
   },
   "outputs": [],
   "source": [
    "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0hzukDBgVom"
   },
   "source": [
    "**The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.**\n",
    "\n",
    "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVxS8OPI9uI0"
   },
   "outputs": [],
   "source": [
    "#tf.linalg.band_part(\n",
    "    #input,\n",
    "    #num_lower,\n",
    "    #num_upper,\n",
    "    #name=None\n",
    "#)\n",
    "# num_lower: number of subdiagonals to keep. If negative, keeps, entire triangle. \n",
    "# num_upper: # of upper diagonals to keep. If negative, keeps entire triangle. \n",
    "\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxKGuXxaBeeE"
   },
   "outputs": [],
   "source": [
    "x = tf.random.uniform((1, 3))\n",
    "print('x', x)\n",
    "temp = create_look_ahead_mask(x.shape[1])\n",
    "print('mask', temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XyxA5wUssmC8"
   },
   "outputs": [],
   "source": [
    "print(tf.matmul(x,temp))\n",
    "x+=temp\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xluDl5cXYy4y"
   },
   "source": [
    "## Scaled dot product attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsxEE_-Wa1gF"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
    "\n",
    "The attention function used by the transformer takes three inputs: **Q (query), K (key), V (value).** The equation used to calculate the attention weights is:\n",
    "\n",
    "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
    "\n",
    "The dot-product attention is scaled by a factor of square root of the depth. **This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax.** \n",
    "\n",
    "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
    "\n",
    "**The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LazzUq3bJ5SH"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask, dec_timestep, K=None, V=None, Z=None, mode='self'): \n",
    "  \n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., num_particles, depth) for sampled word\n",
    "    k: key shape == (..., num_particles, depth) for sampled word\n",
    "    v:value shape == (..., num_particles, depth_v) for sampled word\n",
    "    \n",
    "    K: key shape == (..., num_particles, seq_len_k, depth)\n",
    "    V: value shape == (..., num_particles, seq_len_v, depth_v)\n",
    "    Z: value shape == (..., num_particles, seq_len_v, depth)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., num_particles, seq_len_q, seq_len_k). Defaults to None.\n",
    "    mode: to distinct between 'encoder-decoder' (no SMC for now) attention & self-attention \n",
    "    \n",
    "  Returns:\n",
    "    output (new Z), attention_weights, K, V\n",
    "  \"\"\"\n",
    "\n",
    "  # FOR SMC: SHAPE OF Q (..., NUM_PARTICLES, seq_len_q, depth)\n",
    "             # SHAPE OF K (..., NUM_PARTICLES, seq_len_k, depth)\n",
    "            # SHAPE OF V (..., NUM_PARTICLES, seq_len_v, depth_v)\n",
    "      \n",
    "  # FOR SMC: k[l]=K0:k[l], v[l]=v0:k[l], q[l]=q[Il]     \n",
    "  if K is not None:\n",
    "    # compute K(0:k) from K(0:k-1) & k\n",
    "    # dim of K in the case of multi-head attention: (batch_size, num_particles, num_heads, seq_length, Depth)\n",
    "    K=tf.concat([K[:,:,:,:dec_timestep,:], k, tf.expand_dims(K[:,:,:,dec_timestep+1,:],axis=3)], axis=3)\n",
    "  else:\n",
    "    K=k\n",
    "  if V is not None:\n",
    "    # compute the V(0:k) with V(0:k-1) & v\n",
    "    V=tf.concat([V[:,:,:,:dec_timestep,:], v, tf.expand_dims(V[:,:,:,dec_timestep+1,:],axis=3)], axis=3)\n",
    "  else:\n",
    "    V=v\n",
    "  \n",
    "  # adapt shape of q for doing the matmul\n",
    "  matmul_qk = tf.matmul(q, K, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9) \n",
    "    # ok, makes sense: mask multiplied by a number closed to -infinity (to zero out after softmax) and sum with scaled_attention_logits \n",
    "    #(because sum means product after softmax)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., num_particles, seq_len_q, seq_len_k)\n",
    "  \n",
    "  output = tf.matmul(attention_weights, V)\n",
    "  # here, output= z_k[l] -> shape (..., num_particles, depth)\n",
    "  \n",
    "  if mode=='self' and Z is not None:\n",
    "    # compute the Z(0:k) from z(0:k-1) & output\n",
    "    Z[:,:,dec_timestep,:]=output\n",
    "  else:\n",
    "    Z=output\n",
    "  \n",
    "  # The output return the variable Z(0:k)\n",
    "  # FOR THE SMC TRANSFORMER, THE OUTPUT SHAPE SHOULD BE (..., NUM_PARTICLES, seq_len_q, depth_v) \n",
    "  \n",
    "  return (Z,K,V), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n90YjClyInFy"
   },
   "outputs": [],
   "source": [
    "def print_out(q, k, v):\n",
    "  (temp_out, temp_K, temp_V), temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None, 3)\n",
    "  print ('Attention weights are:')\n",
    "  print (temp_attn)\n",
    "  print ('Output is:')\n",
    "  print (temp_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yAzUAf2DPlNt"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zg6k-fGhgXra"
   },
   "outputs": [],
   "source": [
    "# This query aligns with a repeated key (third and fourth), \n",
    "# so all associated values get averaged.\n",
    "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAq3YOzUgXhb"
   },
   "outputs": [],
   "source": [
    "# This query aligns equally with the first and second key, \n",
    "# so their values get averaged.\n",
    "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aOz-4_XIhaTP"
   },
   "source": [
    "Pass all the queries together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dlU8Tm-hYrF"
   },
   "outputs": [],
   "source": [
    "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmzGPEy64qmA"
   },
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz5BMC8Kaoqo"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
    "\n",
    "\n",
    "Multi-head attention consists of four parts:\n",
    "*    Linear layers and split into heads.\n",
    "*    Scaled dot-product attention.\n",
    "*    Concatenation of heads.\n",
    "*    Final linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JPmbr6F1C-v_"
   },
   "source": [
    "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
    "\n",
    "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). **An appropriate mask must be used in the attention step.**  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
    "\n",
    "**Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSV3PPKsYecw"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  '''\n",
    "  multi-head attention mechanism for each layer of the Transformer\n",
    "  -args: \n",
    "    -d_model: depth model\n",
    "    -num_heads: number of heads for the multi-head attention mechanism\n",
    "    -num_particles: number of particles to generate \n",
    "    -dec_timestep: current decoding timestep (=k) for the sequential mechanism\n",
    "    -mode: self-attention (default) or encoder/decoder attention\n",
    "    '''\n",
    "  def __init__(self, d_model, num_heads, num_particles, dec_timestep, mode='self'): # 2 arguments added: dec_timestep, mode. \n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    # distinct between encoder-decoder attention (no SMC) & self-attention (w/ SMC):\n",
    "    self.mode=mode\n",
    "    self.num_particles=num_particles\n",
    "    self.timestep=dec_timestep\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    (batch_size, num_particle, seq_length, d_model) => (batch_size, num_particle, seq_length, num_heads, depth=d_model/num_heads)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, self.num_particles, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 1, 3, 2, 4])\n",
    "    \n",
    "  def concat_heads(self, x):\n",
    "    '''concat attention parameters over all heads (and permute dimensions)\n",
    "    -returns a tensor of shape (B, P, S, D)'''\n",
    "    scaled_attention = tf.transpose(x, perm=[0, 1, 3, 2, 4])  # (batch_size, NUM_PARTICLES, seq_len_q, num_heads, depth)\n",
    "\n",
    "    return tf.reshape(scaled_attention, \n",
    "                                  (tf.shape(scaled_attention)[0], tf.shape(scaled_attention)[1], -1, self.d_model))  # (batch_size, NUM_PARTICLES, seq_len_q, d_model)\n",
    "\n",
    "  def call(self, v, k, q, sigma, mask, K=None, V=None, Z=None):\n",
    "    '''\n",
    "    -Args:\n",
    "      -v,k,q: v(k), k(k), q(k): attention parameters (over all heads) @ current decoding timestep. > shape (B,P,D)\n",
    "      - sigma: to compute the Gaussian noise with the reparametrization trick > float tensor of shape (B, P, S, D)\n",
    "      -mask: padding or look_ahead mask. \n",
    "      -K,V,Z: KO:k, V0:k, Z0:k: total length attention parameters (until decoding timestep) > shape (B, P, S, D)\n",
    "    -Returns:\n",
    "      -K:0:k+1, V0:k+1, Z0:k+1\n",
    "      -attention_weights\n",
    "    '''\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    #> FOR SMC: q is only the query of the current word: shape (batch_size, num_particles, d_model)\n",
    "    q = self.wq(q)  # (batch_size, NUM_PARTICLES, seq_len_q, d_model)  \n",
    "    k = self.wk(k)  # (batch_size, NUM_PARTICLES, seq_len_k, d_model)\n",
    "    v = self.wv(v)  # (batch_size, NUM_PARTICLES, seq_len_v, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, NUM_PARTICLES, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, NUM_PARTICLES, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, NUM_PARTICLES, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    if K is not None:\n",
    "      K = self.split_heads(K, batch_size)\n",
    "    if V is not None:\n",
    "      V = self.split_heads(V, batch_size)\n",
    "    if Z is not None:\n",
    "      Z = self.split_heads(Z, batch_size)\n",
    " \n",
    "    # FOR SMC: attention_weights.shape == (batch_size, NUM_PARTICLES, num_heads, seq_len_q, seq_len_k)\n",
    "    (scaled_attention, K, V), attention_weights= scaled_dot_product_attention(q, k, v, mask, self.timestep, K, V, Z, self.mode)\n",
    "    \n",
    "    # concat attention, K, V over all the heads\n",
    "    concat_attention=self.concat_heads(scaled_attention)\n",
    "    concat_K=self.concat_heads(K)\n",
    "    concat_V=self.concat_heads(V)\n",
    "    \n",
    "    # Add gaussian noise here: before or after the dense layer? (test both)\n",
    "    # COMPUTE THE REPARAMETRIZATION TRICK\n",
    "    self.stddev=tf.math.multiply(sigma, tf.random.normal(shape=tf.shape(concat_attention), name='gaussian_noise_reparametrized'))\n",
    "    # replace by a 'fixed' noise for the inference part (with pre-training) > OPTIONAL\n",
    "    #self.gaussian_noise=tf.keras.layers.GaussianNoise(self.stddev)\n",
    "    #output = self.gaussian_noise(self.dense(concat_attention))  # (batch_size, NUM_PARTICLES, seq_len_q, d_model)\n",
    "\n",
    "    output=concat_attention+self.stddev\n",
    "        \n",
    "    # THE OUTPUT IS ALSO THE VARIABLE Z (CONCATENATION OF THE Z OF EACH HEAD)\n",
    "    # FOR SMC: OUTPUT SHAPE (batch_size, NUM_PARTICLES, seq_len_q, d_model)\n",
    "    return (output, concat_K, concat_V), attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0D8FJue5lDyZ"
   },
   "source": [
    "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hu94p-_-2_BX"
   },
   "outputs": [],
   "source": [
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8, num_particles=10, dec_timestep=3, mode='enc-dec')\n",
    "y = tf.random.uniform((1, 10, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "(Z,K,V), attn = temp_mha(y, k=y, q=y, sigma=1, mask=None)\n",
    "#out.shape, attn.shape, K.shape, V.shape\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RdDqGayx67vv"
   },
   "source": [
    "## Point wise feed forward network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBqzJXGfHK3X"
   },
   "source": [
    "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET7xLt0yCT6Z"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, NUM_PARTICLES, seq_len, d_model)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mytb1lPyOHLB"
   },
   "outputs": [],
   "source": [
    "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7e7hKcxn6-zd"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yScbC0MUH8dS"
   },
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfYJG-Kvgwy2"
   },
   "source": [
    "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
    "\n",
    "* **The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.**\n",
    "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6LO_48Owmx_o"
   },
   "source": [
    "### Decoder layer\n",
    "\n",
    "Each decoder layer consists of sublayers:\n",
    "\n",
    "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
    "2.   Point wise feed forward networks\n",
    "\n",
    "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
    "\n",
    "There are N decoder layers in the transformer.\n",
    "\n",
    "**In other words, the decoder predicts the next word by self-attending to its own output.** See the demonstration above in the scaled dot product attention section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-_-F40N0EDC"
   },
   "outputs": [],
   "source": [
    "# original DecoderLayer from TF 2.0 tutorial on Tranformer\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  '''adaptated version of the original Decoder Layer of the Transformer. \n",
    "  The only difference are the shapes of the input tensor (B, P, S, D) instead of (B, S, D)\n",
    "  -args:\n",
    "    -d_model: model depth\n",
    "    -num_heads: number of heads in the multi-head attention mechanism\n",
    "    -dff: output dimension of the feed forward network\n",
    "    -num_particles: number of simulated particles for the latent state space of the Transformer\n",
    "    -rate: dropout rate for output layers\n",
    "  '''\n",
    "\n",
    "  def __init__(self, d_model, num_heads, dff, num_particles, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "    \n",
    "    self.dec_timestep=0\n",
    "    self.num_particles=num_particles\n",
    "    \n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads, num_particles, self.dec_timestep, mode='self')\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "  def set_decoder(self, decoder):\n",
    "    '''trick because this function is present in the DecoderLayer_SMC class.'''\n",
    "    pass\n",
    "    \n",
    "  def call(self, x, training, look_ahead_mask, padding_mask):\n",
    "    '''\n",
    "    -args: \n",
    "        -x: \n",
    "        -training: boolean to distinct between training and evaluation phase. \n",
    "        -look_ahead_mask: for masking future decoding timestep\n",
    "        -padding_mask: for fixed-size words sequence. \n",
    "\n",
    "    -returns\n",
    "        -r0:k: entire sequence (until current decoding timestep) output of the Decoder layer > dim (B, P, S, D)\n",
    "        -attention_weights_block: useful to visualize attention \n",
    "    '''\n",
    "    (Z,K,V), attn_weights_block1 = self.mha1(x, x, x, 0, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(Z, training=training)\n",
    "    out1 = self.layernorm1(Z + x)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out1)  # (batch_size, target_seq_len, d_model)\n",
    "    return out3, attn_weights_block1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x3zAjV_zzDVn"
   },
   "outputs": [],
   "source": [
    "sample_decoder_layer = DecoderLayer(512, 8, 2048, 10)\n",
    "\n",
    "sample_decoder_layer_output, _,= sample_decoder_layer(\n",
    "    tf.random.uniform((64, 10, 50, 512)),\n",
    "    False, None, None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9SoX0-vd1hue"
   },
   "outputs": [],
   "source": [
    "# Code for the Decoder Layer with SMC. \n",
    "class DecoderLayer_SMC(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, sigma,  num_particles, layer_num, decoder=None, rate=0.1):\n",
    "    '''\n",
    "    -Args: \n",
    "      -d_model: model depth\n",
    "      -num_heads: number of heads in the multi-head attention mechanism\n",
    "      -dff: output dimension of the feed forward network\n",
    "      -target_vocab_size (for computing the sampling weights)\n",
    "      -maximum_position_encoding: number of positions for positional encodings. \n",
    "      -sigma: constant noise used for the Gaussian Noise with the reparametrization trick > float tensor of shape (B, P, S, D) # CODE IT AS A LEARNED PARAMETER INSTEAD. \n",
    "      -num_particles: number of simulated particles for the latent state space of the Transformer\n",
    "      -layer_num: only used if resampling is done at layer-level (in the Decoder class)\n",
    "      -rate: dropout rate for output layers\n",
    "    '''\n",
    "    super(DecoderLayer_SMC, self).__init__()\n",
    "    \n",
    "    # store the decoding timestep\n",
    "    self.dec_timestep=0\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads, num_particles, self.dec_timestep, mode='self')\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    self.pos_encoding_SMC=positional_encoding_SMC(maximum_position_encoding, d_model, num_particles)\n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    self.num_particles=num_particles\n",
    "    self.d_model=d_model\n",
    "    self.target_vocab_size=target_vocab_size\n",
    "\n",
    "    self.layer_num=layer_num\n",
    "    self.decoder=decoder\n",
    "\n",
    "    # This layer needs to share weights with the Transformer.final_layer\n",
    "    if self.decoder is None:\n",
    "      print('SMC mechanism done at decoder-level...')\n",
    "      self.output_layer=tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    self.initialize=False\n",
    "\n",
    "  def set_decoder(self, decoder):\n",
    "    '''only used is the resampling is done at layer level.'''\n",
    "    self.decoder=decoder\n",
    "\n",
    "  def forward_pass_from_layer(self, x):\n",
    "    '''compute the forward pass from the layer until the decoder output\n",
    "      only used if re-sampling is done at layer level.\n",
    "    '''\n",
    "    #if self.decoder is not None:\n",
    "        #forward_layers=self.decoder.dec_layers[self.layer_num:]\n",
    "        #for layer in forward_layers:\n",
    "          #forward_func=layer()\n",
    "    #else:\n",
    "    forward_func=self.output_layer\n",
    "    return forward_func(x)\n",
    "\n",
    "  def initialize_indices_matrix(self, batch_size, seq_length):\n",
    "    # TO COMPUTE AT TRANSFORMER LEVEL? \n",
    "    ind_matrix=tf.zeros(shape=(batch_size, self.num_particles, seq_length),dtype=tf.int32)\n",
    "    self.ind_matrix=ind_matrix\n",
    "    return ind_matrix # tf.stop_gradient(ind_matrix)? \n",
    "    \n",
    "  def preprocess_words(self, x, training):\n",
    "    '''add words embeddings and positional encodings:\n",
    "        -Args: \n",
    "          -x: 3D tensor of words sequence > dim (B, S, dim_words)\n",
    "          -training: boolean for dropout\n",
    "        -Returns: \n",
    "          - A 3D tensor of pre-processed words sequence > dim (B, S, D)\n",
    "    '''\n",
    "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # division by the root of the d_model\n",
    "    # addition of the positional encoding to the input x for the current decoding step:\n",
    "    #x += self.pos_encoding_SMC[:,:,self.dec_timestep, :] # dim of positional encoding (1, num_particles,  num_positions, d_model)\n",
    "    x += self.pos_encoding[:,self.dec_timestep, :] # dim of positional encoding (1, num_positions, d_model)\n",
    "    x = self.dropout(x, training=training)\n",
    "    return x\n",
    "    \n",
    "  def sample_previous_word(self, indices, prev_Z, training):\n",
    "    '''sample Xk-1[Ik[l]] the embedding of a selected word at the previous position, with probabilities given by the last layer output\n",
    "    Args:\n",
    "      -prev_Z: Z0:k-1 > dim (B, P, S, D)\n",
    "      -indices: used to recompute Zk-1 > dim (B,P)\n",
    "      -training: boolean for dropout \n",
    "    Returns:\n",
    "      - the batch of sampled word of shape (B, P, 1, D)\n",
    "    '''\n",
    "    # recompute Z0:k-1 with the set of indices\n",
    "    if len(indices.shape)==3:\n",
    "      indices=tf.squeeze(indices, axis=-1)\n",
    "    \n",
    "    # compute z(k-1) with resampling \n",
    "    Z_previousk=tf.gather(prev_Z[:,:,self.dec_timestep,:],indices, axis=1, batch_dims=1)\n",
    "    # compute Z0:k-1\n",
    "    if self.dec_timestep==0:\n",
    "      prev_Z=tf.concat([tf.expand_dims(Z_previousk,axis=2),prev_Z[:,:,self.dec_timestep+1:,:]], axis=2)\n",
    "    else:\n",
    "      prev_Z_left=prev_Z[:,:,:self.dec_timestep,:]\n",
    "      prev_Z=tf.concat([prev_Z_left,tf.expand_dims(Z_previousk,axis=2),prev_Z[:,:,self.dec_timestep+1:,:]], axis=2)\n",
    "  \n",
    "    #compute the log probabilities associated to the prediction at Z0:k-1\n",
    "    sample_words_id=[]\n",
    "    predictions_probas=self.forward_pass_from_layer(Z_previousk) # dimensions (batch_size, num_particles, vocabulary_size)\n",
    "\n",
    "    # select a word id randomly with proba equal to predictions_probas. \n",
    "    for n in range(self.num_particles):\n",
    "      # TRY TO ELIMINATE THIS FOR LOOP\n",
    "      sample_words_id+=[tf.random.categorical(predictions_probas[:,n,:], num_samples=1)]\n",
    "\n",
    "    sample_words_id=tf.concat(sample_words_id, axis=1) # dimensions (B, P)\n",
    "    sample_words_id=tf.expand_dims(sample_words_id, axis=-1) # adding the seq_length dimension\n",
    "    # > dim (B, P, 1)\n",
    "\n",
    "    #preprocess the words with the word embedding & positional encoding.\n",
    "    x=self.preprocess_words(sample_words_id, training) # dim (B, P, 1)\n",
    "    return x\n",
    "  \n",
    "  def sample_and_keep_indices(self, prev_sampling_weights, ind_matrix): # add a mask argument?\n",
    "    '''samples the set of N indices for doing the weights resampling\n",
    "    adds this set of indices to the matrix of ancestor indices\n",
    "    Args:\n",
    "    -prev_Z: Z0:k-1 > dim (B, P, S, D)\n",
    "    -prev_sampling_weights: w(k-1) > dim (B, P, V) or (B, P)? \n",
    "    -indice matrix: I0:k-1 > dim (B, P, S)\n",
    "    Returns:\n",
    "    -The current set of indices to do a forward pass on the Decoder Layer > dim (batch_size, num_particles)\n",
    "    -The updated ancestor indices matrix > dim (batch_size, NUM_PARTICLES, seq_length)'''\n",
    "\n",
    "    # generate a uniform distribution between 0 and 1 \n",
    "    unif_distrib=tf.random.uniform(shape=tf.shape(prev_sampling_weights), maxval=1)\n",
    "    # add a tf.stop_gradient(prev_sampling_weights)? \n",
    "\n",
    "    # use the function compute ancestral_index\n",
    "    #indices=self.compute_ancestral_index(prev_sampling_weights, unif_distrib)\n",
    "\n",
    "    # Sample current set of indices with proba proportional to prev_sampling_weights \n",
    "    indices=tf.random.categorical(prev_sampling_weights, self.num_particles) # shape (..., num_particles)\n",
    "\n",
    "    # Add this set of indices to the indices matrix tensor: \n",
    "    indices=tf.cast(indices, tf.int32)\n",
    "    indices=tf.expand_dims(indices, axis=-1)\n",
    "    updated_ind_matrix=tf.concat([ind_matrix[:,:,:self.dec_timestep],indices,ind_matrix[:,:,self.dec_timestep+1:]],axis=-1)\n",
    "  \n",
    "    return indices, updated_ind_matrix # tf.stop_gradient(indices), tf.stop_gradient(updated_ind_matrix)\n",
    "\n",
    "  def compute_ancestral_index(self, prev_sampling_weights, uniform_distribution):\n",
    "    '''\n",
    "    -args: \n",
    "      -prev_sampling_weights: float tensor of shape (B,P)\n",
    "      -uniform distribution: float tensor of dimension (B,P)\n",
    "    -returns:\n",
    "      -the current set of M indices > tensor of shape (B,P)\n",
    "    '''\n",
    "    batch_size=tf.shape(prev_sampling_weights)[0]\n",
    "    num_particles=tf.shape(prev_sampling_weights)[1]\n",
    "\n",
    "    # compute w_bar:\n",
    "    W_0=tf.expand_dims(tf.constant(0,dtype=tf.float32, shape=(batch_size,)), axis=-1)\n",
    "    W_m=[tf.reduce_sum(prev_sampling_weights[:,:m], axis=-1) for m in range(num_particles)]\n",
    "    W_m=tf.stack(W_m, axis=-1)\n",
    "    W_m=tf.concat([W_0, W_m], axis=-1)\n",
    "\n",
    "    indices_func=np.zeros(shape=tf.shape(prev_sampling_weights))\n",
    "\n",
    "    # use tf.random.categorical (check if the same)\n",
    "\n",
    "    # TRY TO REMOVE THIS DOUBLE FOR LOOP!!! > see this github repo as an example: \n",
    "    #https://github.com/rlabbe/filterpy/blob/master/filterpy/monte_carlo/resampling.py\n",
    "    for b in range(batch_size): \n",
    "      for i in range(num_particles):\n",
    "        unif=uniform_distribution[b,i]\n",
    "        if unif>=W_m[b,i] and unif<=W_m[b,i+1]:\n",
    "          indices_func[b,i]=i+1  \n",
    "\n",
    "    indices_func=tf.convert_to_tensor(indices_func, dtype=tf.int32)\n",
    "    \n",
    "    ancestral_index=tf.stack([tf.reduce_sum(indices_func[:,:i], axis=-1) for i in range(num_particles)], axis=-1)\n",
    "    ancestral_index=tf.cast(ancestral_index, tf.int32)\n",
    "\n",
    "    return ancestral_index # tf.stop_gradient(ancestral_index)\n",
    "\n",
    "  def call(self, x, PREV_SAMPL_WEIGHTS, K, V, TARGET_WORD_ID, training, \n",
    "           look_ahead_mask, padding_mask, previous_word=None):\n",
    "    '''\n",
    "    -args: \n",
    "        -x: input tensor of the multi-head attention mechanism (Z0:k-1)\n",
    "        -prev_sampling_weights:\n",
    "        -K: K0:k-1\n",
    "        -V: VO:k-1\n",
    "        -Target_word_ID: to compute the new set of sampling weights > dim (B,)\n",
    "        -training: for dropout\n",
    "        -look_ahead_mask: to mask the future decoding timesteps\n",
    "        -padding_mask: to have fixed-size words sequence. \n",
    "        -previous_word: used only if are not sampling a word for computing attention but taking directly the previous word. \n",
    "    -returns: \n",
    "        -attention vectors (Z0:k, K0:k, V0:k)\n",
    "        -sampling_weights wk\n",
    "        -indices matrix I0:k\n",
    "        -attention_weights_block: for attention vizualisation \n",
    "    '''\n",
    "    \n",
    "    # check if the decoder layer has been initialized. \n",
    "    # to remove? initialization done at Transformer level\n",
    "           \n",
    "    \n",
    "    # FOR SMC: 1. SAMPLE THE SET OF N INDICES USING THE sample_indices function\n",
    "    if self.dec_timestep==0:\n",
    "      self.initialize_indices_matrix(tf.shape(K)[0], tf.shape(K)[2]) # TO REMOVE (OR NOT) INITIALIZATION DONE AT TRANSFORMER LEVEL. \n",
    "    \n",
    "    indices, self.ind_matrix=self.sample_and_keep_indices(PREV_SAMPL_WEIGHTS, self.ind_matrix)\n",
    "    \n",
    "    # 2. Using the sampled indices, get the set of N sampled previous embedded words using the function 'sample_previous_words'\n",
    "    if previous_word is None:\n",
    "      # sampling method\n",
    "      sample_word=self.sample_previous_word(indices, x, training) # dim (B, P, 1, D)\n",
    "    else:\n",
    "      # greedy method\n",
    "      sample_word=self.preprocess_words(previous_word)\n",
    "    \n",
    "    # compute the self-attention vectors over x\n",
    "    # tensors of dim (B,P,S,D)\n",
    "    (attn1, K, V), attn_weights_block1= self.mha1(sample_word, sample_word, sample_word, sigma, look_ahead_mask, K, V)\n",
    "\n",
    "    # RESAMPLE TRAJECTORIES OF ATTENTION PARAMETERS FROM THE INDICES MATRIX AND THE CURRENT SET OF WEIGHTS.  \n",
    "    attn1=tf.gather(attn1, self.ind_matrix, batch_dims=1)\n",
    "    attn1=tf.squeeze(attn1, axis=3)\n",
    "    K=tf.gather(K, self.ind_matrix, batch_dims=1)\n",
    "    V=tf.gather(V, self.ind_matrix, batch_dims=1)\n",
    "\n",
    "    # (batch_size, NUM_PARTICLES, target_seq_len, d_model)\n",
    "    attn1 = self.dropout1(attn1, training=training)\n",
    "    out1 = self.layernorm1(attn1 + x) # ERROR OF SHAPES HERE: here x shouldn't be the previous word but the whole (masked) sequence of words (or the previous_Z)\n",
    "\n",
    "    ffn_output = self.ffn(out1)  # (batch_size, NUM_PARTICLES, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out3 = self.layernorm3(ffn_output + out1)  # (batch_size, NUM_PARTICLES, target_seq_len, d_model)\n",
    "    \n",
    "    # 3. FOR SMC: compute the new set of weights.\n",
    "    if len(tf.shape(TARGET_WORD_ID))==1:\n",
    "      TARGET_WORD_ID=tf.expand_dims(TARGET_WORD_ID, axis=-1)\n",
    "    predictions_probas=self.output_layer(out3) # (batch_size, NUM_PARTICLES, target_seq_len, target_vocab_size)\n",
    "    sampling_weights=tf.gather(predictions_probas[:,:,self.dec_timestep,:], TARGET_WORD_ID, axis=-1, batch_dims=1)\n",
    "    #sampling_weights=predictions_probas[:,:,self.dec_timestep,TARGET_WORD_ID]\n",
    "    \n",
    "    # add tf.stop_gradient here? \n",
    "\n",
    "    self.dec_timestep+=1 # right place to put it? \n",
    "    \n",
    "    # FOR SMC: RETURN ADDITIONAL PARAMETERS, THE CURRENT SAMPLING WEIGHTS (wk[l]), THE ANCESTOR INDICES MATRIX, K, AND V. \n",
    "    return (out3, K, V), sampling_weights, self.ind_matrix, attn_weights_block1 \n",
    "    # or here tf.stop_gradient(sampling_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ne2Bqx8k71l0"
   },
   "outputs": [],
   "source": [
    "d_model=512\n",
    "num_heads=8\n",
    "dff=2048\n",
    "target_vocab_size=1000\n",
    "num_particles=10\n",
    "max_positional_encoding=5000\n",
    "sigma=1\n",
    "\n",
    "sample_decoder_layer = DecoderLayer_SMC(d_model, num_heads, dff,target_vocab_size, max_positional_encoding, sigma, num_particles, layer_num=0)\n",
    "\n",
    "PREV_SAMPL_WEIGHTS=tf.random.uniform(shape=(64,num_particles), maxval=1)\n",
    "K=tf.random.uniform((64, num_particles, 50, 512))\n",
    "V=tf.random.uniform((64, num_particles, 50, 512))\n",
    "TARGET_WORD_ID=tf.constant(34, shape=(64,1))\n",
    "\n",
    "(sample_decoder_layer_output, K, V), sampling_weights, ind_matrix, _ = sample_decoder_layer(\n",
    "    tf.random.uniform((64, 10, 50, 512)), PREV_SAMPL_WEIGHTS, K, V, TARGET_WORD_ID, training=False, look_ahead_mask=None, padding_mask=None)\n",
    "\n",
    "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZMFqrcczByi0"
   },
   "outputs": [],
   "source": [
    "#print(sampling_weights)\n",
    "print(sampling_weights.shape)\n",
    "#print(ind_matrix)\n",
    "print(ind_matrix.shape)\n",
    "#print(K)\n",
    "print(K.shape)\n",
    "# print(V)\n",
    "print(V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p-uO6ls8m2O5"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtT7PKzrXkNr"
   },
   "source": [
    " The `Decoder` consists of:\n",
    "1.   Output Embedding\n",
    "2.   Positional Encoding\n",
    "3.   N decoder layers\n",
    "\n",
    "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d5_d5-PLQXwY"
   },
   "outputs": [],
   "source": [
    "# class Decoder that takes a SMC Decoder Layers as input. \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  '''Class Decoder with the Decoder architecture\n",
    "  -args\n",
    "    -num_layers: number of layers in the Decoder\n",
    "    -d_model: model depth \n",
    "    -num_heads: number of heads in the multi-attention mechanism\n",
    "    -dff: output dim of the feedforward network\n",
    "    -target_vocab_size (for computing the sampling weights for the last layer (or all layers))\n",
    "    -maxixum_position_encoding: to preprocess the words sequence (addition of positional embeddings)\n",
    "    -sigma: constant noise for the Gaussian Noise model with reparametrization trick > float tensor of shape (B, P, S, D)\n",
    "    -PF_algo: decoder-level (default) or layer-level\n",
    "    -rate: dropout rate for feed-forward layers. \n",
    "    '''\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "               maximum_position_encoding, sigma,  num_particles, PF_algo='decoder-level', rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "    self.num_particles=num_particles\n",
    "    self.sigma=sigma\n",
    "    self.PF_algo=PF_algo\n",
    "    \n",
    "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "    self.pos_encoding_SMC=positional_encoding_SMC(maximum_position_encoding, d_model, num_particles)\n",
    "\n",
    "\n",
    "    # build the decoder architecture\n",
    "\n",
    "    # for layer-level PF resampling mechanism: list of layers= DecoderLayer_SMC layers\n",
    "    if PF_algo=='layer-level':\n",
    "      print('building the Decoder with SMC mechanism done at layer level...')\n",
    "      self.dec_layers = [DecoderLayer_SMC(d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, sigma, num_particles, l) \n",
    "                       for l in range(num_layers)]\n",
    "\n",
    "\n",
    "    # for decoder-level PF resampling mechanism: \n",
    "    # list of layers= N-1 DecoderLayer layers for the first N-1 layers + one DecoderLayer_SMC as the top layer. \n",
    "    elif PF_algo=='decoder-level':\n",
    "      print('building the Decoder with SMC mechanism done at decoder level...')\n",
    "      self.dec_layers=[DecoderLayer(d_model, num_heads, dff, num_particles) \n",
    "                       for _ in range(num_layers-1)]+[DecoderLayer_SMC(d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, sigma, num_particles, num_layers)]\n",
    "    else:\n",
    "      assert False, \"Invalid PF_algo: should be: layer-level/decoder-level\"\n",
    "    \n",
    "    self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    self.dec_timestep=0\n",
    "    self.output_layer=self.dec_layers[self.num_layers-1].output_layer\n",
    "    self.decoder_initialized=False\n",
    "\n",
    "\n",
    "  def set_decoder_inside_layers(self, decoder):\n",
    "    '''only useful for layer-level resampling mechanism'''\n",
    "    for i in range(self.num_layers):\n",
    "      self.dec_layers[i].set_decoder(decoder)\n",
    "    self.decoder_initialized=True\n",
    "\n",
    "  def preprocess_words_input(self, x, training):\n",
    "    '''pre_process sequence of words by adding embeddings + positional encoding\n",
    "      -Args:\n",
    "        -x: 3D tensor for the input sequence of words > dim (B, P, S, d_input) OR a 2D tensor of dim (B, P, S) (word_id instead of words...)\n",
    "        -training: boolean for dropout\n",
    "      -Returns:\n",
    "        -A 3D tensor of the pre-processed sequence of words > dim (B, S, D)\n",
    "    '''\n",
    "    seq_len = tf.shape(x)[2]\n",
    "    if len(tf.shape(x))==3:\n",
    "      x = self.embedding(x) # (batch_size, num_particles, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # division by the root of the d_model\n",
    "    x += self.pos_encoding_SMC[:,:, :seq_len, :] # addition of the positional encoding to the input x\n",
    "    x = self.dropout(x, training=training)\n",
    "    return x\n",
    "    \n",
    "  def call(self, x, PREV_SAMPLING_WEIGHTS, K, V, TARGET_WORD_ID, training, \n",
    "           look_ahead_mask, padding_mask, previous_word=None):\n",
    "    '''\n",
    "    -args:\n",
    "      -x: input of the first decoder layer (X0:k-1)\n",
    "      -prev_sampling_weights: wk-1\n",
    "      -K: K0:k-1\n",
    "      -V: V0k-1\n",
    "      -TARGET_WORD_ID: to compute resampling weights wk > shape (B,)\n",
    "      -training\n",
    "      -look_ahead_mask\n",
    "      -padding_mask\n",
    "      -previous_word: used if self.attn_method='greedy',\n",
    "    -returns: \n",
    "      -attention vectors (Z[final_layer]0:k, K[final_layer]0:k, V[final_layer]0:k)\n",
    "      -sampling weights wk\n",
    "      -ancestor indices matrix I0:k\n",
    "      -attention_weights: for attention visualization\n",
    "    '''\n",
    "    attention_weights = {}\n",
    "    \n",
    "    if self.PF_algo=='layer-level':\n",
    "      for i in range(self.num_layers):\n",
    "        # PF @ layer-level: compute attention vectors and sampling_weights for each layer\n",
    "        # PREV_SAMPLING_WEIGHTS (from previous decoding step or sampling_weights from previous layer?)\n",
    "        (x,K,V), sampling_weights, ind_matrix, block2 = self.dec_layers[i](x, PREV_SAMPLING_WEIGHTS, K, V, TARGET_WORD_ID, training,\n",
    "                                               look_ahead_mask, padding_mask, previous_word)\n",
    "\n",
    "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "    elif self.PF_algo=='decoder-level':\n",
    "      # do the pre_processing step for x (not included at layer level)\n",
    "      x=self.preprocess_words_input(x, training)\n",
    "      for i in range(self.num_layers-1):\n",
    "        # No PF & resampling mechanism @ the first N-1 layers\n",
    "        x, block2=self.dec_layers[i](x, training, look_ahead_mask, padding_mask)\n",
    "        attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "      # PF mechanism & resampling @ the last layer.\n",
    "      (x,K,V), sampling_weights, ind_matrix, block2 = self.dec_layers[-1](x, PREV_SAMPLING_WEIGHTS, K, V, TARGET_WORD_ID, training,\n",
    "                                               look_ahead_mask, padding_mask, previous_word)\n",
    "    else:\n",
    "      assert False, \"Invalid PF_algo: should be: layer-level/decoder-level\"\n",
    "      \n",
    "    attention_weights['decoder_layer{}_block2'.format(self.num_layers+1)] = block2\n",
    "      \n",
    "    self.dec_timestep+=1\n",
    "    \n",
    "    # x.shape == (batch_size, target_seq_len, d_model)\n",
    "    return (x,K,V), tf.stop_gradient(sampling_weights), tf.stop_gradient(ind_matrix), attention_weights # tf.stop_gradient(sampling_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1jXoAMRZyvu"
   },
   "outputs": [],
   "source": [
    "num_particles=5\n",
    "sample_decoder = Decoder(num_layers=2, d_model=64, num_heads=8, \n",
    "                         dff=2048, target_vocab_size=1000,\n",
    "                         maximum_position_encoding=5000, sigma=1, num_particles=5, PF_algo='decoder-level')\n",
    "\n",
    "\n",
    "PREV_SAMPL_WEIGHTS=tf.random.uniform(shape=(64,num_particles), maxval=1)\n",
    "K=tf.random.uniform((64, num_particles, 20, 64))\n",
    "V=tf.random.uniform((64, num_particles, 20, 64))\n",
    "TARGET_WORD_ID=tf.constant(10, shape=(64,1))\n",
    "Z=tf.random.uniform((64, num_particles, 20, 64))\n",
    "\n",
    "(output, K, V), final_sampling_weights, ind_matrix, attn = sample_decoder(Z, PREV_SAMPL_WEIGHTS,\n",
    "                              K,V, TARGET_WORD_ID,\n",
    "                              training=False, look_ahead_mask=None, \n",
    "                              padding_mask=None)\n",
    "\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y54xnJnuYgJ7"
   },
   "source": [
    "## Create the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uERO1y54cOKq"
   },
   "source": [
    "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PED3bIpOYkBu"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "  '''class for the Transformer Model\n",
    "  -args\n",
    "    -num_layers: number of decoder layers\n",
    "    -d_model: model_depth\n",
    "    -num_heads: number of heads in the multi-head attention mechanism. \n",
    "    -dff: output dimension of the feed-forward layer. \n",
    "    -target_vocab_size:for computing the resampling weights\n",
    "    -pe_target: maximum_positional_encoding\n",
    "    -num_particles: number of particles generated. \n",
    "    -sigma: constant noise for the Gaussian Noise (reparametrization trick) > float tensor of shape (B, P, S, D). \n",
    "    -PF_algo: decoder-level/ layer-level > PF resampling mechanism done @ decoder-level (only on last layer) or layer level.\n",
    "    -attn_method: greedy (use previous for computing attention vectors) or sampling (sample a word with proba equal to log_probas)\n",
    "    -rate: dropout rate for the feed-forward layer. \n",
    "    '''\n",
    "  def __init__(self, num_layers, d_model, num_heads, dff, \n",
    "               target_vocab_size, pe_target, num_particles, sigma,  PF_algo='layer-level', attn_method='sampling', rate=0.1):\n",
    "    super(Transformer, self).__init__()\n",
    "\n",
    "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
    "                           target_vocab_size, pe_target, sigma, num_particles, PF_algo, rate)\n",
    "    \n",
    "    #get the output layer of the last decoder layer as final layer. \n",
    "    #self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    self.final_layer=self.decoder.dec_layers[num_layers-1].output_layer\n",
    "\n",
    "    self.num_particles=num_particles\n",
    "    self.d_model=d_model\n",
    "\n",
    "    self.initialize=False\n",
    "\n",
    "  def preprocess_words(self, x, dec_timestep, training):\n",
    "    '''add words embeddings and positional encodings:\n",
    "        -Args: \n",
    "          -x: 3D tensor of words sequence > dim (B, S, dim_words)\n",
    "          -training: boolean for dropout\n",
    "        -Returns: \n",
    "          - A 3D tensor of pre-processed words sequence > dim (B, S, D)\n",
    "    '''\n",
    "    x = self.decoder.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) # division by the root of the d_model\n",
    "    # addition of the positional encoding to the input x for the current decoding step:\n",
    "    #x += self.pos_encoding_SMC[:,:,self.dec_timestep, :] # dim of positional encoding (1, num_particles,  num_positions, d_model)\n",
    "    x += self.decoder.pos_encoding[:,dec_timestep, :] # dim of positional encoding (1, num_positions, d_model)\n",
    "    x = self.decoder.dropout(x, training=training)\n",
    "    return tf.reshape(x, shape=[tf.shape(x)[0], tf.shape(x)[2], tf.shape(x)[1], tf.shape(x)[-1]])\n",
    "\n",
    "  def initialize_attn_SMC_parameters(self, batch_size, seq_length, initial_word_tensor):\n",
    "    ''' initialize the attention parameters of the Transformer\n",
    "          -Args: \n",
    "            -batch_size\n",
    "            -seq_length: longueur of input sequence of words \n",
    "            -initial_word_tensor: 1D tensor of dim (batch_size) with the initial words for each element of the batch. \n",
    "            Used to compute the initial set of weights \n",
    "          -Returns      dtype=int32)>\n",
    " \n",
    "            -Z0, K0, V0 (dim (B,P,S,D)) W0 (dim (B,P)), initial indices matrix (dim (B, P, S))\n",
    "    '''\n",
    "    # initialize K0, V0, Z0 (=V0)\n",
    "    K=tf.random.uniform(shape=(batch_size, self.num_particles, seq_length, self.d_model), maxval=1)\n",
    "    V=tf.random.uniform(shape=(batch_size, self.num_particles, seq_length, self.d_model), maxval=1)\n",
    "    Z=V\n",
    "    # initialize w0\n",
    "    log_probas=self.decoder.dec_layers[0].forward_pass_from_layer(Z) # shape (B, P, S, V)\n",
    "    log_probas_initial=log_probas[:,:,0,:]\n",
    "    initial_word_tensor=tf.expand_dims(initial_word_tensor, axis=-1)\n",
    "    initial_weights=tf.squeeze(tf.gather(log_probas_initial, initial_word_tensor, axis=-1, batch_dims=1),axis=-1)\n",
    "    # tf.stop_gradient(initial_weights)? \n",
    "\n",
    "    # call the initialization of the ancestor indices matrix\n",
    "    ind_matrix_init=self.decoder.dec_layers[-1].initialize_indices_matrix(batch_size, seq_length)\n",
    "    # tf.stop_gradient(ind_matrix_init)? \n",
    "\n",
    "    self.initialize=True\n",
    "\n",
    "    return (Z, K, V), initial_weights, ind_matrix_init\n",
    "\n",
    "  def compute_average_weighted_output(self, decoder_output, resampling_weights):\n",
    "      '''\n",
    "      -Args:\n",
    "        - decoder_output: current output of the decoder (M particles) > dims (B, P, D)\n",
    "        - resampling_weights: output sampling weights of the decoder > dims (B,P)\n",
    "      -Returns:\n",
    "        - the weighted average of the decoder's M particles > dims (B,D)\n",
    "      '''\n",
    "      # USELESS ACTUALLY HERE. \n",
    "      mean_output = tf.squeeze(tf.matmul(decoder_output, resampling_weights, transpose_a=True))\n",
    "      sum_weights = tf.tile(tf.expand_dims(tf.reduce_sum(resampling_weights, axis=-1), axis=-1), \n",
    "                            [1, mean_output.shape[-1], 1])\n",
    "      sum_weights = tf.reduce_sum(resampling_weights, axis=1)\n",
    "      mean_output= mean_output / sum_weights\n",
    "      return mean_output\n",
    "    \n",
    "  def call(self, input_tensor, prev_sampling_weights, K, V, targets, training, \n",
    "           look_ahead_mask, dec_padding_mask):\n",
    "    # remove prev_sampling_weights, K, V (initialize them instead)\n",
    "    # target_word_id here is instead the targets tensor (tensor shift from one compared to the input tensor)\n",
    "    # add a decoding_timestep argument? \n",
    "    '''\n",
    "    -args:\n",
    "      -input tensor: transformer input data : sequence of words id. > shape (B,S)\n",
    "      -targets: target tensor\n",
    "      -training: for dropout layers\n",
    "      -look_ahead_mask: \n",
    "      -dec_padding_mask: \n",
    "    -returns\n",
    "      -final_output: Y0:S > shape (?, P, S, V)\n",
    "      -attention_weights: for visualisation\n",
    "    '''\n",
    "\n",
    "    # initialize the attention parameters\n",
    "    batch_size=tf.shape(input_tensor)[0]\n",
    "    seq_length=tf.shape(input_tensor)[1]\n",
    "    # get initial_word_id instead of initial_word? \n",
    "    (Z0, K0, V0), w0, I0=self.initialize_attn_SMC_parameters(batch_size, seq_length, input_tensor[:,0])\n",
    "    # tf.stop_gradient(w0)? \n",
    "\n",
    "    # add the function 'set_decoder_inside_layers?\n",
    "    self.decoder.set_decoder_inside_layers(self.decoder) # DOES THIS WORK???\n",
    "\n",
    "    # do a loop over decoding_timestep...\n",
    "    for t in range(tf.shape(targets)[1]): \n",
    "      target_word_id=targets[:,t]\n",
    "      sampling_weights=w0\n",
    "      K=K0\n",
    "      V=V0\n",
    "      print('decoding timestep:', t)\n",
    "\n",
    "      if len(tf.shape(input_tensor)) < 4: \n",
    "        # transform the 2D input_tensor (dim (B,S)) into a 4D input_tensor (dim (B,P,S,D))\n",
    "        input_tensor=tf.expand_dims(input_tensor, axis=-1)\n",
    "        input_tensor=self.preprocess_words(input_tensor, t, training=training) # dim (B, P, S)\n",
    "        input_tensor=tf.tile(input_tensor, multiples=[1,self.num_particles,1,1])\n",
    "\n",
    "      # dec_output.shape == (batch_size, NUM_PARTICLES, tar_seq_len, d_model)\n",
    "      (dec_output, K, V), sampling_weights, ind_matrix, attention_weights= self.decoder(\n",
    "        input_tensor, sampling_weights, K, V, target_word_id, training, look_ahead_mask, dec_padding_mask)\n",
    "      \n",
    "      # compute the predicted word from the decoder output\n",
    "      # REPLACE BY TEACHER FORCING\n",
    "      # cf tutorial notebook on NMt with attention:\n",
    "      #dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "      final_output = self.final_layer(dec_output)  # (batch_size, NUM_PARTICLES, tar_seq_len, target_vocab_size)\n",
    "      current_dec_output=final_output[:,:,t,:]\n",
    "      average_output=self.compute_average_weighted_output(current_dec_output, sampling_weights) # dims (B, V) # LIGN TO DEBUG. \n",
    "    \n",
    "      # concatentate the predicted_id to the input_tensor which is given to the decoder\n",
    "      # as its input.\n",
    "      # find the predicted_word and add the word embedding\n",
    "      predicted_word=tf.cast(tf.argmax(average_output, axis=-1), tf.int32)\n",
    "      predicted_word = self.decoder.embedding(tf.expand_dims(predicted_word, axis=-1))\n",
    "      predicted_word=tf.tile(tf.expand_dims(predicted_word, axis=1), [1,self.num_particles,1,1])\n",
    "\n",
    "      # add it to the input_tensor\n",
    "      input_tensor=tf.cast(input_tensor, dtype=tf.float32)\n",
    "      input_tensor=tf.concat([input_tensor[:,:,:t+1,:], predicted_word, input_tensor[:,:,t+2:,:]], axis=2)\n",
    "\n",
    "    return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJ4fbQcIkHW1"
   },
   "outputs": [],
   "source": [
    "sample_transformer = Transformer(\n",
    "    num_layers=2, d_model=512, num_heads=8, dff=2048, sigma=1, target_vocab_size=8000, \n",
    "    pe_target=6000, num_particles=10)\n",
    "\n",
    "temp_data =tf.cast(tf.zeros(shape=(64,51)), dtype=tf.int32)\n",
    "input_tensor=temp_data[:,:51]\n",
    "targets=temp_data[:, 1:]\n",
    "\n",
    "sample_transformer.initialize\n",
    "\n",
    "PREV_SAMPL_WEIGHTS=tf.random.uniform(shape=(64,num_particles), maxval=1)\n",
    "K=tf.random.uniform((64, num_particles, 50, 512))\n",
    "V=tf.random.uniform((64, num_particles, 50, 512))\n",
    "\n",
    "fn_out, _ = sample_transformer(input_tensor, PREV_SAMPL_WEIGHTS, K, V, targets, training=False, \n",
    "                               look_ahead_mask=None,\n",
    "                               dec_padding_mask=None)\n",
    "\n",
    "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wsINyf1VEQLC"
   },
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVjWCxFNcgbt"
   },
   "source": [
    "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
    "\n",
    "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
    "\n",
    "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lnJn5SLA2ahP"
   },
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = tokenizer_pt.vocab_size + 2\n",
    "target_vocab_size = tokenizer_en.vocab_size + 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axfDsEw4Yy0N"
   },
   "source": [
    "## additional hyperparameters for the SMC mechanism\n",
    "#### Gaussian Noise\n",
    "* initialization of the standard deviation\n",
    "* exact 'place' where the gaussian noise is added \n",
    "\n",
    "#### number of particles \n",
    "\n",
    "#### attention parameters computation\n",
    "* sampling a word with proba proportional to log probabilites of previous timestep (sampling method) versus using directly the previous word (greedy method). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AbTeT9zae5zt"
   },
   "source": [
    "# 2. TRAINING OF THE TRANSFORMER MODEL. (to modify for the SMC transformer). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zW51n7peiuRF"
   },
   "source": [
    "### Custom training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NnRXzAVJi_tu"
   },
   "outputs": [],
   "source": [
    "def loss(predicted_y, target_y):\n",
    "  return tf.reduce_mean(tf.square(predicted_y - target_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV7htqr4i9Pi"
   },
   "outputs": [],
   "source": [
    "def train(model, inputs, outputs, learning_rate):\n",
    "  with tf.GradientTape() as t:\n",
    "    current_loss = loss(model(inputs), outputs)\n",
    "  dW, db = t.gradient(current_loss, [model.W, model.b])\n",
    "  model.W.assign_sub(learning_rate * dW)\n",
    "  model.b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WO605Kq5jRE_"
   },
   "outputs": [],
   "source": [
    "def train(model, inputs, outputs, learning_rate):\n",
    "  with tf.GradientTape() as t:\n",
    "    current_loss = loss(model(inputs), outputs)\n",
    "  dW, db = t.gradient(current_loss, [model.W, model.b])\n",
    "  model.W.assign_sub(learning_rate * dW)\n",
    "  model.b.assign_sub(learning_rate * db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w27oXzObjTP_"
   },
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# Collect the history of W-values and b-values to plot later\n",
    "Ws, bs = [], []\n",
    "epochs = range(10)\n",
    "for epoch in epochs:\n",
    "  Ws.append(model.W.numpy())\n",
    "  bs.append(model.b.numpy())\n",
    "  current_loss = loss(model(inputs), outputs)\n",
    "\n",
    "  train(model, inputs, outputs, learning_rate=0.1)\n",
    "  print('Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f' %\n",
    "        (epoch, Ws[-1], bs[-1], current_loss))\n",
    "\n",
    "# Let's plot it all\n",
    "plt.plot(epochs, Ws, 'r',\n",
    "         epochs, bs, 'b')\n",
    "plt.plot([TRUE_W] * len(epochs), 'r--',\n",
    "         [TRUE_b] * len(epochs), 'b--')\n",
    "plt.legend(['W', 'b', 'True W', 'True b'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYEGhEOtzn5W"
   },
   "source": [
    "## Optimizer\n",
    "### TO MODIFY FOR SMC WITH A CUSTOM GRADIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GOmWW--yP3zx"
   },
   "source": [
    "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
    "\n",
    "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYQdOO1axwEI"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7r4scdulztRx"
   },
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f33ZCgvHpPdG"
   },
   "outputs": [],
   "source": [
    "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
    "\n",
    "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YgkDE7hzo8r5"
   },
   "source": [
    "## Loss and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxGJtoDuYIHL"
   },
   "source": [
    "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlhsJMm0TW_B"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67oqVHiT0Eiu"
   },
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "  # TO CHANGE WITH A CUSTOM LOSS\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  # keep the loss above and add the 'left part' of the language_models.tex\n",
    "  \n",
    "  return tf.reduce_mean(loss_)\n",
    "\n",
    "  # check if you compute a loss at layer levl (can be done in TF2) and sum over layers\n",
    "  # or do a global loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_rjeZ3oI4i9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNVw7w929gxN"
   },
   "outputs": [],
   "source": [
    "def loss_left_part_SMC(real, pred):\n",
    "  #see formula on .tex file\n",
    "  #to compute the determinant: https://www.tensorflow.org/api_docs/python/tf/linalg/det\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "phlyxMnm-Tpx"
   },
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
    "    name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeHumfr7zmMa"
   },
   "source": [
    "## Training and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiysUa--4tOU"
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
    "                          input_vocab_size, target_vocab_size, \n",
    "                          pe_input=input_vocab_size, \n",
    "                          pe_target=target_vocab_size,\n",
    "                          rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZOJUSB1T8GjM"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "  # Encoder padding mask\n",
    "  enc_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 2nd attention block in the decoder.\n",
    "  # This padding mask is used to mask the encoder outputs.\n",
    "  dec_padding_mask = create_padding_mask(inp)\n",
    "  \n",
    "  # Used in the 1st attention block in the decoder.\n",
    "  # It is used to pad and mask future tokens in the input received by \n",
    "  # the decoder.\n",
    "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "  dec_target_padding_mask = create_padding_mask(tar)\n",
    "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "  return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fzuf06YZp66w"
   },
   "source": [
    "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNhuYfllndLZ"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Di_Yaa1gf9r"
   },
   "source": [
    "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
    "\n",
    "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
    "\n",
    "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
    "\n",
    "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
    "\n",
    "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
    "\n",
    "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
    "\n",
    "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKpoA6q1sJFj"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJwmp9OE29oj"
   },
   "outputs": [],
   "source": [
    "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
    "# execution. The function specializes to the precise shape of the argument\n",
    "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
    "# batch sizes (the last batch is smaller), use input_signature to specify\n",
    "# more generic shapes.\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions, _ = transformer(inp, tar_inp, \n",
    "                                 True, \n",
    "                                 enc_padding_mask, \n",
    "                                 combined_mask, \n",
    "                                 dec_padding_mask)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables) # DOES THIS NEED TO CHANGE?  \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qM2PDWGDJ_8V"
   },
   "source": [
    "Portuguese is used as the input language and English is the target language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bbvmaKNiznHZ"
   },
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_states()\n",
    "  train_accuracy.reset_states()\n",
    "  \n",
    "  # inp -> portuguese, tar -> english\n",
    "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNVUiQo5j83R"
   },
   "source": [
    "# 3. EVALUATION/INFERENCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mI2yf1lWkUaZ"
   },
   "source": [
    "![formula for inference](https://colab.research.google.com/drive/1K61J3rRbBrJW9dhy24jpwHPiKCKUluTP#scrollTo=mI2yf1lWkUaZ/content/IMG_7002.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6mt0A6bqPWG"
   },
   "source": [
    "### The prediction loop (from text generation with RNN)\n",
    "\n",
    "The following code block generates the text:\n",
    "\n",
    "* It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
    "\n",
    "* Get the prediction distribution of the next character using the start string and the RNN state.\n",
    "\n",
    "* Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character as our next input to the model.\n",
    "\n",
    "* The RNN state returned by the model is fed back into the model so that it now has more context, instead than only one word. After predicting the next word, the modified RNN states are again fed back into the model, which is how it learns as it gets more context from the previously predicted words.\n",
    "\n",
    "\n",
    "Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s3HwxK-zqmkS"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "  \n",
    "  # TO MODIFY USING THE SMC TRANSFORMER MODEL. \n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyLkOFllqv0d"
   },
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfcsSWswSdGV"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y6APsFrgImLW"
   },
   "source": [
    "The following steps are used for evaluation:\n",
    "\n",
    "* **Encode the input sentence using the Portuguese tokenizer (`tokenizer_pt`). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.**\n",
    "* **The decoder input is the `start token == tokenizer_en.vocab_size`.**\n",
    "* Calculate the padding masks and the look ahead masks.\n",
    "* **The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).**\n",
    "* Select the last word and calculate the argmax of that.\n",
    "* **Concatentate the predicted word to the decoder input as pass it to the decoder.**\n",
    "* **In this approach, the decoder predicts the next word based on the previous words it predicted.**\n",
    "\n",
    "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5buvMlnvyrFm"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  start_token = [tokenizer_pt.vocab_size]\n",
    "  end_token = [tokenizer_pt.vocab_size + 1]\n",
    "  \n",
    "  # inp sentence is portuguese, hence adding the start and end token\n",
    "  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n",
    "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
    "  \n",
    "  # as the target is english, the first word to the transformer should be the\n",
    "  # english start token.\n",
    "  decoder_input = [tokenizer_en.vocab_size]\n",
    "  output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "  for i in range(MAX_LENGTH):\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
    "        encoder_input, output)\n",
    "  \n",
    "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
    "    predictions, attention_weights = transformer(encoder_input, \n",
    "                                                 output,\n",
    "                                                 False,\n",
    "                                                 enc_padding_mask,\n",
    "                                                 combined_mask,\n",
    "                                                 dec_padding_mask)\n",
    "    \n",
    "    # select the last word from the seq_len dimension\n",
    "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "    \n",
    "    # return the result if the predicted_id is equal to the end token\n",
    "    if predicted_id == tokenizer_en.vocab_size+1:\n",
    "      return tf.squeeze(output, axis=0), attention_weights\n",
    "    \n",
    "    # concatentate the predicted_id to the output which is given to the decoder\n",
    "    # as its input.\n",
    "    output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output, axis=0), attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CN-BV43FMBej"
   },
   "outputs": [],
   "source": [
    "def plot_attention_weights(attention, sentence, result, layer):\n",
    "  fig = plt.figure(figsize=(16, 8))\n",
    "  \n",
    "  sentence = tokenizer_pt.encode(sentence)\n",
    "  \n",
    "  attention = tf.squeeze(attention[layer], axis=0)\n",
    "  \n",
    "  for head in range(attention.shape[0]):\n",
    "    ax = fig.add_subplot(2, 4, head+1)\n",
    "    \n",
    "    # plot the attention weights\n",
    "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 10}\n",
    "    \n",
    "    ax.set_xticks(range(len(sentence)+2))\n",
    "    ax.set_yticks(range(len(result)))\n",
    "    \n",
    "    ax.set_ylim(len(result)-1.5, -0.5)\n",
    "        \n",
    "    ax.set_xticklabels(\n",
    "        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n",
    "        fontdict=fontdict, rotation=90)\n",
    "    \n",
    "    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n",
    "                        if i < tokenizer_en.vocab_size], \n",
    "                       fontdict=fontdict)\n",
    "    \n",
    "    ax.set_xlabel('Head {}'.format(head+1))\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lU2_yG_vBGza"
   },
   "outputs": [],
   "source": [
    "def translate(sentence, plot=''):\n",
    "  result, attention_weights = evaluate(sentence)\n",
    "  \n",
    "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
    "                                            if i < tokenizer_en.vocab_size])  \n",
    "\n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n",
    "  \n",
    "  if plot:\n",
    "    plot_attention_weights(attention_weights, sentence, result, plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YsxrAlvFG8SZ"
   },
   "outputs": [],
   "source": [
    "translate(\"este é um problema que temos que resolver.\")\n",
    "print (\"Real translation: this is a problem we have to solve .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7EH5y_aqI4t1"
   },
   "outputs": [],
   "source": [
    "translate(\"os meus vizinhos ouviram sobre esta ideia.\")\n",
    "print (\"Real translation: and my neighboring homes heard about this idea .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J-hVCTSUMlkb"
   },
   "outputs": [],
   "source": [
    "translate(\"vou então muito rapidamente partilhar convosco algumas histórias de algumas coisas mágicas que aconteceram.\")\n",
    "print (\"Real translation: so i 'll just share with you some stories very quickly of some magical things that have happened .\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1MxkSZvz0jX"
   },
   "source": [
    "You can pass different layers and attention blocks of the decoder to the `plot` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-kFyiOLH0xg"
   },
   "outputs": [],
   "source": [
    "translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n",
    "print (\"Real translation: this is the first book i've ever done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RqQ1fIsLwkGE"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, you learned about positional encoding, multi-head attention, the importance of masking and how to create a transformer.\n",
    "\n",
    "Try using a different dataset to train the transformer. You can also create the base transformer or transformer XL by changing the hyperparameters above. You can also use the layers defined here to create [BERT](https://arxiv.org/abs/1810.04805) and train state of the art models. Futhermore, you can implement beam search to get better predictions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SMC_transformer_forward_model_AM_w/o_encoder.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
