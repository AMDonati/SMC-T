INFO:training log:model hyperparameters from the config file: {'num_layers': 1, 'num_heads': 1, 'd_model': 3, 'dff': 12, 'rate': 0.1, 'maximum_position_encoding_baseline': 50, 'maximum_position_encoding_smc': 'None'}
INFO:training log:smc hyperparameters from the config file: {'num_particles': 25, 'noise_encoder': 'False', 'noise_SMC_layer': 'True', 'sigma': 0.1}
INFO:training log:starting the training of the smc transformer...
INFO:training log:number of training samples: 336290
INFO:training log:steps per epoch: 320
WARNING:tensorflow:Layer smc__transformer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

INFO:training log:Epoch 1/20
INFO:training log:final weights of first 3 elements of batch: [0.04036092 0.04033617 0.04003531 0.03960523 0.04017294 0.0405368
 0.03945211 0.04047689 0.04023252 0.03987063 0.03967817 0.0392484
 0.03995296 0.03962149 0.04001546 0.03992252 0.04095037 0.0390822
 0.04007949 0.04026127 0.04019634 0.04044599 0.04002478 0.03960434
 0.03983667], [0.04007442 0.04060176 0.03943161 0.04091388 0.03605192 0.03893771
 0.04038634 0.0409865  0.0397766  0.04040259 0.04052177 0.03963536
 0.03881637 0.04096609 0.04102893 0.04094283 0.03905851 0.04099193
 0.03727758 0.040896   0.04035921 0.03911032 0.04102029 0.04078213
 0.04102939], [0.04000423 0.03633394 0.04141834 0.0381762  0.04072942 0.04022544
 0.04003134 0.04088323 0.04271553 0.03917467 0.0406849  0.04025765
 0.03931747 0.04186965 0.04166442 0.03859054 0.04054466 0.03981113
 0.04048515 0.03888761 0.03963125 0.03843176 0.03964272 0.04143837
 0.03905036]
INFO:training log:train loss 6.556102752685547 -  train mse loss: 0.5585605502128601 - val loss: 6.41855525970459 - val mse loss: 0.4102459251880646
INFO:training log:Time taken for 1 epoch: 2663.5064628124237 secs

INFO:training log:Epoch 2/20
INFO:training log:final weights of first 3 elements of batch: [0.03999817 0.04007999 0.04003301 0.04001082 0.04003489 0.03996837
 0.03976606 0.04005155 0.04004822 0.03996335 0.04014203 0.03999054
 0.03985235 0.04016374 0.03992782 0.04004569 0.03987322 0.0401261
 0.04011421 0.04017167 0.03970267 0.04005767 0.04003194 0.04016746
 0.03967838], [0.04072395 0.04098267 0.03971609 0.04135396 0.04086008 0.04013504
 0.04077834 0.04030054 0.03898868 0.04069826 0.03981715 0.03959115
 0.03999235 0.04010851 0.03784272 0.03924454 0.03973917 0.0397028
 0.03951341 0.03904257 0.04030893 0.04080526 0.0394592  0.039769
 0.04052561], [0.04031309 0.0406556  0.03976736 0.04087571 0.0402227  0.04041242
 0.04051096 0.04023533 0.0388447  0.04016175 0.04012985 0.04002073
 0.04056584 0.03995625 0.03919029 0.04081331 0.03969686 0.04040936
 0.04067825 0.03921957 0.03864365 0.03988546 0.03898619 0.03923233
 0.04057245]
INFO:training log:train loss 6.275644302368164 -  train mse loss: 0.27468517422676086 - val loss: 6.2245683670043945 - val mse loss: 0.2234678715467453
INFO:training log:Time taken for 1 epoch: 2650.876085281372 secs

INFO:training log:Epoch 3/20
INFO:training log:final weights of first 3 elements of batch: [0.03999871 0.03992241 0.04000695 0.04010166 0.04006454 0.04007366
 0.03997417 0.04008377 0.04010738 0.0400691  0.0401099  0.03994581
 0.04005712 0.0399914  0.040007   0.04011023 0.03975844 0.04000698
 0.0399775  0.04004335 0.03994383 0.03985973 0.03990705 0.04005956
 0.03981969], [0.0400332  0.04025562 0.04061211 0.04075478 0.03947205 0.04004194
 0.04040438 0.04006996 0.03974829 0.04034043 0.04024116 0.03951793
 0.03983099 0.03903267 0.03959607 0.040406   0.04006411 0.04020552
 0.04037023 0.03971437 0.03952543 0.03921041 0.04025775 0.04016498
 0.04012966], [0.03812498 0.04078135 0.04050931 0.04132595 0.04035826 0.03984561
 0.04053739 0.04040172 0.03902195 0.04011941 0.03889761 0.04003337
 0.04024705 0.04108671 0.03878886 0.0383845  0.04179531 0.04089115
 0.0399718  0.03685272 0.03987022 0.03860739 0.04015618 0.04127134
 0.04211982]
INFO:training log:train loss 6.175241470336914 -  train mse loss: 0.17689359188079834 - val loss: 6.1649861335754395 - val mse loss: 0.1667426973581314
INFO:training log:Time taken for 1 epoch: 2641.210839033127 secs

INFO:training log:Epoch 4/20
INFO:training log:final weights of first 3 elements of batch: [0.04060111 0.04004311 0.0400596  0.04014562 0.039639   0.0399357
 0.03977639 0.04006116 0.04035398 0.03954662 0.03944231 0.04049024
 0.04051272 0.04008753 0.03947345 0.04008264 0.04000232 0.039079
 0.03984193 0.0406531  0.04014102 0.03966454 0.040243   0.0398157
 0.04030821], [0.03852975 0.039602   0.04073816 0.03834517 0.03822584 0.04127846
 0.03893542 0.04097228 0.03967837 0.04032961 0.03802018 0.04073259
 0.04150024 0.04046016 0.04046317 0.04029714 0.04119408 0.04240497
 0.03672722 0.03946526 0.03913096 0.04198963 0.03934508 0.04161786
 0.04001639], [0.0380775  0.0391245  0.03779608 0.04039824 0.04218044 0.04017496
 0.04148629 0.04143685 0.04165929 0.0419999  0.04247562 0.04005754
 0.03825086 0.03622186 0.04059639 0.03864274 0.0404413  0.03872825
 0.04223721 0.03852985 0.04042063 0.0420604  0.04075894 0.03573107
 0.04051326]
INFO:training log:train loss 6.0703325271606445 -  train mse loss: 0.07298479974269867 - val loss: 6.02690315246582 - val mse loss: 0.03015254996716976
INFO:training log:Time taken for 1 epoch: 2645.2708644866943 secs

INFO:training log:Epoch 5/20
INFO:training log:final weights of first 3 elements of batch: [0.04056206 0.04073654 0.038851   0.04052155 0.04041469 0.03976382
 0.0396545  0.04002725 0.03981742 0.03976938 0.03990462 0.03968567
 0.03958323 0.04022074 0.04019466 0.03998252 0.04021317 0.04023582
 0.03989229 0.03959666 0.04069379 0.03975978 0.04031046 0.04016188
 0.03944653], [0.04156968 0.04072587 0.04287215 0.04060592 0.03966347 0.03928522
 0.0381997  0.0368879  0.04001851 0.04143995 0.04006593 0.03967172
 0.03933258 0.03949054 0.04033042 0.03978629 0.04033586 0.03811264
 0.0432344  0.04009264 0.03903812 0.03950873 0.03826997 0.04120325
 0.04025852], [0.03682901 0.03959276 0.03950232 0.04031093 0.03848811 0.03760757
 0.0412853  0.04270065 0.04149981 0.04237642 0.03612381 0.04207763
 0.04093    0.04024651 0.0400777  0.04235705 0.03901449 0.03845868
 0.04041852 0.03960024 0.03726916 0.04060816 0.04184191 0.03934202
 0.04144129]
INFO:training log:train loss 6.018476486206055 -  train mse loss: 0.01672939583659172 - val loss: 6.012526035308838 - val mse loss: 0.011644504964351654
INFO:training log:Time taken for 1 epoch: 2643.5796535015106 secs

INFO:training log:Epoch 6/20
INFO:training log:final weights of first 3 elements of batch: [0.04006512 0.04029444 0.03939781 0.04035031 0.04031558 0.0401726
 0.04008953 0.03975827 0.03979066 0.03991807 0.0403269  0.03937341
 0.03989991 0.04017846 0.04039019 0.04024465 0.03992089 0.03994852
 0.0393127  0.04054146 0.0395279  0.04030652 0.04042244 0.03989981
 0.03955381], [0.04066668 0.04087349 0.0410796  0.03992959 0.03965335 0.03826465
 0.04097791 0.04123879 0.04285086 0.0409523  0.0395497  0.0395825
 0.0404034  0.03877728 0.03682963 0.04101036 0.03875679 0.04081531
 0.03920409 0.039722   0.03750171 0.04108246 0.03934767 0.04028214
 0.04064777], [0.03999154 0.04142305 0.04006707 0.0397925  0.03786116 0.04139223
 0.04140038 0.041463   0.03842993 0.04061104 0.04146365 0.03940691
 0.03900205 0.03914442 0.03977708 0.0413451  0.0408745  0.038449
 0.03936916 0.03610943 0.04172991 0.0406453  0.03966217 0.04060835
 0.03998105]
INFO:training log:train loss 6.011922359466553 -  train mse loss: 0.011063165962696075 - val loss: 6.003374099731445 - val mse loss: 0.007833191193640232
INFO:training log:Time taken for 1 epoch: 2644.8949942588806 secs

INFO:training log:Epoch 7/20
INFO:training log:final weights of first 3 elements of batch: [0.03870166 0.03993519 0.04008634 0.04003091 0.03995744 0.03961156
 0.04067352 0.0405471  0.04030278 0.03934808 0.04000091 0.0396137
 0.03996496 0.03953009 0.04016558 0.04003082 0.03979509 0.03995116
 0.04031378 0.03998212 0.04014934 0.04048254 0.04040935 0.04000896
 0.04040698], [0.04058854 0.03971322 0.03960178 0.04100538 0.03976963 0.03938359
 0.03976748 0.0396643  0.04129709 0.04029949 0.04045108 0.04074631
 0.04092066 0.03854759 0.04016287 0.03930301 0.04206713 0.03956928
 0.03989157 0.03889436 0.04001719 0.0406006  0.04023459 0.0383378
 0.03916549], [0.03992709 0.03890261 0.04124434 0.03803407 0.04096402 0.03900481
 0.03952551 0.03807695 0.03850123 0.03965795 0.03896818 0.03981755
 0.03978888 0.0428308  0.03952239 0.03927808 0.04036838 0.04069312
 0.04099429 0.04226997 0.04056155 0.03932282 0.03968139 0.04120593
 0.04085805]
INFO:training log:train loss 6.005450248718262 -  train mse loss: 0.008383767679333687 - val loss: 6.010525703430176 - val mse loss: 0.006225200369954109
INFO:training log:Time taken for 1 epoch: 2645.058515071869 secs

INFO:training log:Epoch 8/20
INFO:training log:final weights of first 3 elements of batch: [0.03990557 0.03996556 0.03990601 0.03969922 0.04002519 0.04024391
 0.04042595 0.04002535 0.03975627 0.03970218 0.04027434 0.03960876
 0.04019592 0.0399166  0.0400175  0.03998072 0.04002475 0.04020292
 0.03991549 0.04013405 0.03997875 0.03955961 0.04035186 0.04004101
 0.04014246], [0.03868422 0.03939225 0.03962783 0.03904526 0.0412603  0.04232548
 0.04037344 0.03917164 0.0412093  0.03916295 0.04011102 0.03979218
 0.03909434 0.03989635 0.04031052 0.0405313  0.03854218 0.03966635
 0.03932916 0.0408949  0.04108175 0.04083328 0.04024441 0.04023369
 0.03918593], [0.04013173 0.03974648 0.0405501  0.03977466 0.0403597  0.0407587
 0.04051256 0.04157673 0.03984887 0.03823469 0.03916127 0.03909547
 0.04149253 0.03990228 0.04038702 0.04187188 0.03931569 0.03769034
 0.03864114 0.04069131 0.04135771 0.03970911 0.0403596  0.03943531
 0.03939519]
INFO:training log:train loss 6.010813236236572 -  train mse loss: 0.006886306684464216 - val loss: 5.9983134269714355 - val mse loss: 0.005171620287001133
INFO:training log:Time taken for 1 epoch: 2646.72749876976 secs

INFO:training log:Epoch 9/20
INFO:training log:final weights of first 3 elements of batch: [0.03988446 0.04034415 0.04028793 0.03965941 0.04008267 0.03981823
 0.03979462 0.03964297 0.03979244 0.03988666 0.0402737  0.04008022
 0.03984496 0.03967931 0.03994766 0.04038399 0.03986754 0.0402907
 0.04011723 0.04008486 0.04017071 0.04011242 0.03984523 0.03999265
 0.04011529], [0.04040263 0.04083505 0.04062004 0.03935247 0.03897393 0.04058696
 0.04031165 0.04059146 0.03801762 0.04090847 0.04024595 0.04018294
 0.03974508 0.0408892  0.04078992 0.04018412 0.03954194 0.03913837
 0.04008076 0.03972778 0.03988494 0.04002371 0.03971964 0.03925432
 0.03999108], [0.03956855 0.04014675 0.0396799  0.04021476 0.03985649 0.04154124
 0.04030236 0.03980632 0.04059811 0.03981279 0.04034032 0.04073915
 0.04183433 0.03981757 0.03981397 0.04026394 0.038237   0.03923991
 0.04052199 0.03808631 0.0397095  0.03992335 0.04069314 0.03940831
 0.03984391]
INFO:training log:train loss 6.010234355926514 -  train mse loss: 0.006495180539786816 - val loss: 6.003993511199951 - val mse loss: 0.004697555676102638
INFO:training log:Time taken for 1 epoch: 2647.3030173778534 secs

INFO:training log:Epoch 10/20
INFO:training log:final weights of first 3 elements of batch: [0.03980559 0.04059635 0.0395753  0.04008259 0.04006046 0.03969898
 0.04028226 0.03993563 0.03988238 0.04006422 0.04022207 0.0401986
 0.04009418 0.03977408 0.04015233 0.03981854 0.04016759 0.03992722
 0.04009476 0.03990069 0.04000434 0.03988792 0.04003158 0.04013866
 0.03960367], [0.03913013 0.03902055 0.04014785 0.03966733 0.04033657 0.03943168
 0.03797597 0.03980365 0.0404208  0.04121515 0.03989101 0.04001466
 0.04058855 0.04014638 0.03965326 0.04148323 0.03911044 0.04160815
 0.04026183 0.04115248 0.04007325 0.04021906 0.0391232  0.04000869
 0.03951622], [0.04012473 0.04004401 0.04069528 0.03964906 0.03806533 0.03915689
 0.03902468 0.04068402 0.03976708 0.04089617 0.04090418 0.04086831
 0.04046876 0.03958473 0.04038043 0.03979414 0.04069746 0.03984227
 0.04063216 0.04015435 0.04017022 0.03833083 0.03994394 0.04060277
 0.03951819]
INFO:training log:train loss 6.006535053253174 -  train mse loss: 0.005901448428630829 - val loss: 5.998859405517578 - val mse loss: 0.004416991025209427
INFO:training log:Time taken for 1 epoch: 2646.9661774635315 secs

INFO:training log:Epoch 11/20
INFO:training log:final weights of first 3 elements of batch: [0.03985578 0.03928675 0.0400564  0.03988097 0.04000864 0.04036331
 0.04031227 0.039839   0.0400684  0.0402785  0.03973924 0.03962706
 0.04022361 0.04009765 0.04003752 0.03978473 0.04029055 0.03953659
 0.04000004 0.04008488 0.04034543 0.04021581 0.04023631 0.03973668
 0.04009383], [0.04083242 0.03827553 0.04027271 0.04076592 0.04004092 0.03996702
 0.04024659 0.04041256 0.04103274 0.04006417 0.04124052 0.04013843
 0.03944171 0.03970677 0.04014851 0.03831775 0.04090721 0.04079849
 0.04046304 0.03949828 0.03977125 0.03878251 0.04057321 0.03858292
 0.03971878], [0.03851419 0.0399718  0.04046868 0.04129102 0.03971607 0.04074024
 0.03996747 0.03908963 0.04000895 0.04082392 0.03993515 0.03946337
 0.03938002 0.04025203 0.0392596  0.03883646 0.03931648 0.03965569
 0.04091214 0.04073111 0.04043698 0.04020583 0.04040352 0.04041122
 0.04020847]
INFO:training log:train loss 6.006258010864258 -  train mse loss: 0.005861349869519472 - val loss: 6.0045881271362305 - val mse loss: 0.004204996395856142
INFO:training log:Time taken for 1 epoch: 2652.7210144996643 secs

INFO:training log:Epoch 12/20
INFO:training log:final weights of first 3 elements of batch: [0.03990482 0.04002842 0.0401082  0.03974797 0.03965736 0.04007882
 0.04021529 0.03963807 0.04025413 0.03960055 0.03988375 0.04003895
 0.04004718 0.04028805 0.03977377 0.0398646  0.04044531 0.03995146
 0.04029544 0.04004532 0.03972003 0.03992897 0.04021599 0.0402655
 0.04000211], [0.04069944 0.03986329 0.03996231 0.03957918 0.04068877 0.04041135
 0.03994408 0.04003531 0.04017147 0.04076869 0.04107183 0.0398359
 0.03903751 0.04018258 0.04082446 0.04082087 0.03916545 0.03913607
 0.03993796 0.03932437 0.04071719 0.03912752 0.03892932 0.03922148
 0.0405436 ], [0.03956446 0.03969683 0.03976647 0.04069583 0.04050653 0.04092748
 0.04109101 0.04100193 0.03990303 0.03927684 0.04075615 0.04013707
 0.04036564 0.03930984 0.03888343 0.03913106 0.03933705 0.03912623
 0.03984635 0.04039034 0.04035327 0.04030983 0.04026511 0.04025444
 0.03910368]
INFO:training log:train loss 6.009003162384033 -  train mse loss: 0.005609263200312853 - val loss: 6.00524377822876 - val mse loss: 0.003972106147557497
INFO:training log:Time taken for 1 epoch: 2663.081634283066 secs

INFO:training log:Epoch 13/20
INFO:training log:final weights of first 3 elements of batch: [0.03953793 0.03989495 0.04000815 0.04014514 0.03989951 0.03993261
 0.03995784 0.0398313  0.0401048  0.04017142 0.04025542 0.03982946
 0.03981667 0.03980659 0.04005891 0.0399418  0.0400034  0.04016563
 0.04020377 0.04012654 0.03997586 0.04007771 0.0401396  0.04005668
 0.04005839], [0.0396679  0.04080692 0.03981688 0.03960131 0.04037257 0.03986903
 0.03930656 0.03959982 0.04019195 0.04028599 0.04083688 0.03948085
 0.04050264 0.04047668 0.04104929 0.04009341 0.04011448 0.04062913
 0.04057528 0.03835163 0.03978477 0.03999046 0.03989083 0.03953531
 0.03916945], [0.03965281 0.04140956 0.04014757 0.04061848 0.03989482 0.03906687
 0.03845399 0.04030311 0.03923554 0.04005504 0.04003239 0.04019755
 0.03976858 0.03928356 0.04042929 0.04085178 0.04146522 0.0391348
 0.04004558 0.04015299 0.04026296 0.03999108 0.03942303 0.03978761
 0.04033583]
INFO:training log:train loss 6.009125232696533 -  train mse loss: 0.005013831425458193 - val loss: 6.00142240524292 - val mse loss: 0.003770119044929743
INFO:training log:Time taken for 1 epoch: 2653.4642322063446 secs

INFO:training log:Epoch 14/20
INFO:training log:final weights of first 3 elements of batch: [0.04010433 0.04007307 0.04015254 0.0399843  0.03983281 0.04024594
 0.03976548 0.03988023 0.04032405 0.03996938 0.04001288 0.04016615
 0.04007723 0.03946568 0.04015062 0.03972919 0.04017955 0.0399522
 0.04013301 0.03995776 0.03985829 0.03997537 0.03991731 0.03999111
 0.04010143], [0.04007454 0.039812   0.03997157 0.03986254 0.04044616 0.04008237
 0.04079727 0.03941372 0.04055958 0.0404451  0.04027447 0.04095736
 0.03854645 0.04076729 0.03925955 0.04018168 0.04033088 0.04031772
 0.03913697 0.03972466 0.03974289 0.03981982 0.03869259 0.0404063
 0.04037644], [0.04064502 0.04103941 0.04012956 0.03913489 0.0393708  0.04139145
 0.04015522 0.03910587 0.03917385 0.04086793 0.03968308 0.04069423
 0.03955805 0.03993689 0.04079838 0.03978617 0.04025419 0.0400945
 0.03939674 0.0394328  0.04089864 0.03956091 0.03959272 0.0396143
 0.03968436]
INFO:training log:train loss 6.002995014190674 -  train mse loss: 0.004965967498719692 - val loss: 6.005490303039551 - val mse loss: 0.0039202491752803326
INFO:training log:Time taken for 1 epoch: 2648.105783224106 secs

INFO:training log:Epoch 15/20
INFO:training log:final weights of first 3 elements of batch: [0.04017597 0.04026183 0.04009828 0.04025531 0.03999225 0.04003484
 0.04007781 0.04005616 0.04007505 0.04012362 0.03973392 0.03961195
 0.0401755  0.03981519 0.04010723 0.04004076 0.03992762 0.04012747
 0.03990521 0.040042   0.0399559  0.03932171 0.04005696 0.04011161
 0.03991587], [0.04090371 0.03916037 0.04077616 0.03929096 0.03959686 0.04045054
 0.0401728  0.03966549 0.03948434 0.03986443 0.04024114 0.03952133
 0.04024418 0.04099118 0.04038958 0.04023119 0.04068787 0.03915642
 0.03914969 0.0405746  0.04023917 0.03949284 0.04100367 0.03967788
 0.03903356], [0.04104278 0.04012835 0.04014734 0.03899396 0.04016028 0.03988636
 0.03963174 0.0396771  0.03949807 0.04058558 0.03928755 0.0404499
 0.04031386 0.03939678 0.0397907  0.03960037 0.04112221 0.04030966
 0.03982285 0.04003443 0.03965386 0.04126403 0.03933067 0.04021715
 0.03965447]
INFO:training log:train loss 6.009077548980713 -  train mse loss: 0.0049782865680754185 - val loss: 5.996354103088379 - val mse loss: 0.0036271442659199238
INFO:training log:Time taken for 1 epoch: 2648.65825176239 secs

INFO:training log:Epoch 16/20
INFO:training log:final weights of first 3 elements of batch: [0.03996256 0.04036997 0.04015043 0.04012628 0.04013074 0.04003941
 0.03962706 0.03994301 0.03977793 0.03988119 0.03993848 0.03989117
 0.04021355 0.03988941 0.04009493 0.0397053  0.03996567 0.03993771
 0.04025827 0.03989286 0.03991952 0.04019649 0.04001332 0.03988544
 0.04018931], [0.04005608 0.04048539 0.04051589 0.03970551 0.0399766  0.03974775
 0.03971679 0.0396111  0.03962015 0.03873512 0.04058145 0.03926719
 0.03965553 0.03965488 0.04030709 0.04109805 0.03975669 0.0402446
 0.04018731 0.03981554 0.04024122 0.04005111 0.0400248  0.04107418
 0.03986993], [0.0409436  0.04017171 0.03998687 0.03993985 0.03914304 0.03962431
 0.03941991 0.040893   0.04037111 0.04029391 0.04050367 0.03945233
 0.04076305 0.0396866  0.04058024 0.03952673 0.04044619 0.04037936
 0.04032547 0.03887221 0.03915016 0.04008452 0.04001544 0.03978271
 0.03964401]
INFO:training log:train loss 6.001837253570557 -  train mse loss: 0.00523328548297286 - val loss: 6.000381946563721 - val mse loss: 0.003784965956583619
INFO:training log:Time taken for 1 epoch: 2648.877815246582 secs

INFO:training log:Epoch 17/20
INFO:training log:final weights of first 3 elements of batch: [0.04014684 0.03984429 0.04025667 0.03980792 0.04009893 0.03953387
 0.04014143 0.03976072 0.04026642 0.03963885 0.03997965 0.0398403
 0.04022282 0.0398709  0.04023214 0.03998878 0.03969363 0.04014588
 0.04021219 0.03989836 0.04030997 0.03976438 0.04015904 0.04002434
 0.04016161], [0.03980961 0.0405544  0.03942834 0.03880289 0.04024436 0.04020629
 0.04027849 0.04108989 0.04078127 0.04032828 0.03983408 0.04052963
 0.04073431 0.03893282 0.03955825 0.03970645 0.04027683 0.03930174
 0.03977516 0.0400805  0.04056881 0.03961151 0.04006038 0.03968694
 0.03981877], [0.03974296 0.04018103 0.04032816 0.03993458 0.04096067 0.03968968
 0.03998725 0.04014656 0.04039985 0.03960909 0.04027038 0.04029071
 0.03906599 0.03976805 0.03908547 0.03957359 0.04101302 0.03990881
 0.03994282 0.04019347 0.04045584 0.04007801 0.0405908  0.03957723
 0.03920603]
INFO:training log:train loss 6.003434658050537 -  train mse loss: 0.005044760648161173 - val loss: 6.0023579597473145 - val mse loss: 0.0034829038195312023
INFO:training log:Time taken for 1 epoch: 2647.041843175888 secs

INFO:training log:Epoch 18/20
INFO:training log:final weights of first 3 elements of batch: [0.04009031 0.04006787 0.04017213 0.03998827 0.04010546 0.03997483
 0.04016064 0.04019894 0.03986573 0.0397905  0.04024187 0.03985535
 0.04020207 0.03997215 0.03943658 0.03997201 0.03985466 0.03998576
 0.04014522 0.03966046 0.03995561 0.04025124 0.03993511 0.03985342
 0.04026385], [0.04040268 0.0395464  0.04074923 0.03935225 0.03994557 0.04047595
 0.03945259 0.04056169 0.040068   0.04027173 0.04014637 0.04069841
 0.03995256 0.03956823 0.0406217  0.03975084 0.03957459 0.04009878
 0.03947948 0.04031026 0.03958781 0.03973208 0.03988883 0.0392739
 0.04049012], [0.0394561  0.04077812 0.03979697 0.04038272 0.04005725 0.039634
 0.0400699  0.04055355 0.04024755 0.0406155  0.04025829 0.03965031
 0.04065689 0.03887335 0.03912538 0.04013399 0.03998571 0.04011668
 0.03982958 0.0395066  0.04073414 0.03907932 0.03990234 0.03978707
 0.04076862]
INFO:training log:train loss 5.998441219329834 -  train mse loss: 0.005302575882524252 - val loss: 6.000580787658691 - val mse loss: 0.00360686588101089
INFO:training log:Time taken for 1 epoch: 2647.1712548732758 secs

INFO:training log:Epoch 19/20
INFO:training log:final weights of first 3 elements of batch: [0.0403659  0.04001418 0.03986669 0.03992771 0.04019034 0.0402328
 0.03995197 0.04016653 0.04042371 0.03950792 0.040118   0.04000277
 0.03959659 0.03998197 0.03992685 0.03999723 0.04007205 0.04009183
 0.04002105 0.04000568 0.04003162 0.0396676  0.04007107 0.03985617
 0.03991172], [0.04110604 0.04074897 0.03955314 0.04044526 0.04092363 0.03967329
 0.04020708 0.0400411  0.03908647 0.03880191 0.04041018 0.04009328
 0.03967772 0.03982707 0.03998747 0.03935185 0.04024991 0.04012293
 0.03964446 0.0404339  0.03846715 0.04021308 0.04058418 0.04007223
 0.04027771], [0.04006621 0.03940598 0.04020189 0.04042831 0.04015245 0.04023728
 0.03958533 0.03999757 0.04018051 0.04096344 0.03943931 0.03936858
 0.03933872 0.03988309 0.04042621 0.03971881 0.04071313 0.04003604
 0.04040133 0.04085242 0.04055533 0.03982048 0.03863367 0.03983051
 0.03976341]
INFO:training log:train loss 6.003942966461182 -  train mse loss: 0.00473257340490818 - val loss: 6.003232479095459 - val mse loss: 0.003400481306016445
INFO:training log:Time taken for 1 epoch: 2646.900291442871 secs

INFO:training log:Epoch 20/20
INFO:training log:final weights of first 3 elements of batch: [0.03993793 0.04019847 0.03995989 0.04007044 0.04018354 0.04021164
 0.0398392  0.04031558 0.03959957 0.04000563 0.03988352 0.0398149
 0.03989143 0.03976184 0.04004198 0.04004271 0.0401605  0.04024062
 0.04000382 0.03999443 0.04010979 0.03999345 0.03985642 0.03973335
 0.04014938], [0.04000565 0.04037947 0.04055454 0.03979634 0.03947002 0.03921971
 0.03943112 0.04050354 0.04034589 0.03978342 0.03970452 0.03887046
 0.04005201 0.03985843 0.04062096 0.04017647 0.04006706 0.03933489
 0.0396884  0.04002364 0.04036345 0.04027294 0.04072306 0.04012458
 0.0406294 ], [0.03938707 0.03903033 0.03995528 0.04080109 0.04019393 0.03926921
 0.04092149 0.04047254 0.04090063 0.0400826  0.0405387  0.03927417
 0.03951781 0.03942869 0.03999039 0.04079634 0.03960153 0.04011533
 0.03966053 0.04046556 0.03990787 0.03936652 0.03998029 0.04053551
 0.03980654]
INFO:training log:train loss 6.007711887359619 -  train mse loss: 0.004965880885720253 - val loss: 5.998959064483643 - val mse loss: 0.00344195868819952
INFO:training log:Time taken for 1 epoch: 2646.382717370987 secs

INFO:training log:total training time for 20 epochs:52977.80175733566
INFO:training log:saving loss and metrics information...
INFO:training log:saving model output in .npy files...
INFO:training log:training of SMC Transformer for a time-series dataset done...
INFO:training log:>>>--------------------------------------------------------------------------------------------------------------------------------------------------------------<<<
INFO:training log:model hyperparameters from the config file: {'num_layers': 1, 'num_heads': 1, 'd_model': 3, 'dff': 12, 'rate': 0.1, 'maximum_position_encoding_baseline': 50, 'maximum_position_encoding_smc': 'None'}
INFO:training log:smc hyperparameters from the config file: {'num_particles': 25, 'noise_encoder': 'False', 'noise_SMC_layer': 'True', 'sigma': 0.1}
INFO:training log:starting the training of the smc transformer...
INFO:training log:number of training samples: 336290
INFO:training log:steps per epoch: 320
INFO:training log:Latest checkpoint restored!!
WARNING:tensorflow:Layer smc__transformer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

INFO:training log:starting training after checkpoint restoring from epoch 20
INFO:training log:Epoch 21/30
INFO:training log:final weights of first 3 elements of batch: [0.03923213 0.03960089 0.04000239 0.04158119 0.03954983 0.03763643
 0.04001009 0.04001736 0.04069904 0.03952823 0.04155757 0.04027974
 0.03940208 0.04096325 0.04086163 0.03984648 0.03812636 0.04078465
 0.03960731 0.04011833 0.03979864 0.03986647 0.04020854 0.0410124
 0.03970905], [0.03999083 0.04006642 0.03990922 0.04017833 0.03958977 0.03964512
 0.04002538 0.04030323 0.03975867 0.03998685 0.03999345 0.03975589
 0.03969299 0.04000307 0.04031162 0.04044825 0.03967099 0.04036054
 0.03946939 0.04008034 0.04013979 0.03994234 0.04033368 0.04000934
 0.04033454], [0.04001754 0.03952591 0.04012556 0.03920928 0.04001711 0.04030343
 0.04023804 0.04021712 0.04047489 0.03974189 0.04014441 0.03986268
 0.03995791 0.04045752 0.04025582 0.03973961 0.04002596 0.03971899
 0.04029639 0.0402179  0.04007238 0.03963786 0.03991193 0.03980307
 0.0400268 ]
INFO:training log:train loss 6.004119396209717 -  train mse loss: 0.004984008613973856 - train loss std (mse): 0.0008303805952891707 - val loss: 6.005015850067139 - val mse loss: 0.0034444360062479973 - val loss std (mse): 0.0005305699887685478
INFO:training log:Time taken for 1 epoch: 2699.129977941513 secs

INFO:training log:Epoch 22/30
INFO:training log:final weights of first 3 elements of batch: [0.04087801 0.04078231 0.03909788 0.03981708 0.04059982 0.03935339
 0.03957291 0.03910157 0.04128139 0.03974458 0.03983559 0.03937202
 0.03929363 0.0411283  0.03914863 0.0403591  0.03943931 0.04067791
 0.04132032 0.04113262 0.03907731 0.04028231 0.03956447 0.04114635
 0.03799321], [0.04018437 0.04026981 0.0399047  0.0404035  0.04028334 0.03988996
 0.04027867 0.04018178 0.03958756 0.04026909 0.0399101  0.04000427
 0.03998476 0.04007918 0.03934122 0.03966673 0.03990387 0.03978858
 0.03992921 0.03981714 0.04017391 0.04029284 0.03984455 0.0398617
 0.0401492 ], [0.04024163 0.04018032 0.04003152 0.04043877 0.04005942 0.04022814
 0.04021537 0.04008556 0.03938378 0.03999315 0.03978767 0.03997639
 0.04024982 0.03987852 0.03968749 0.04038521 0.03975888 0.04015202
 0.04035423 0.03965131 0.03951996 0.04001587 0.03987169 0.03971663
 0.04013668]
INFO:training log:train loss 6.005798816680908 -  train mse loss: 0.004641090519726276 - train loss std (mse): 0.0007367073558270931 - val loss: 6.004999160766602 - val mse loss: 0.003412023186683655 - val loss std (mse): 0.0005251684924587607
INFO:training log:Time taken for 1 epoch: 2684.824185848236 secs

INFO:training log:Epoch 23/30
INFO:training log:final weights of first 3 elements of batch: [0.03987734 0.03829613 0.03979869 0.04051337 0.03984375 0.04059022
 0.03948721 0.04151266 0.04103413 0.0413792  0.04173231 0.03850838
 0.04043326 0.04013029 0.03934653 0.04090511 0.03853526 0.04031631
 0.04070662 0.04037621 0.0398101  0.0391208  0.03896185 0.03988852
 0.03889575], [0.03988608 0.04011849 0.0402655  0.04023733 0.03974781 0.04003595
 0.04021972 0.04004494 0.03979853 0.04024572 0.04023647 0.03993373
 0.03985689 0.03961151 0.039848   0.04008377 0.04005139 0.04015044
 0.04012057 0.03991723 0.03982168 0.03969855 0.04001085 0.04007122
 0.03998759], [0.03971065 0.04007424 0.0401077  0.04023682 0.0401358  0.04024346
 0.03991829 0.03984301 0.03987198 0.03978018 0.03980711 0.03979186
 0.03990814 0.04028738 0.03970596 0.03960893 0.04065638 0.04020564
 0.04007963 0.03948816 0.03998924 0.03982713 0.03999011 0.04020901
 0.04052321]
INFO:training log:train loss 6.001763343811035 -  train mse loss: 0.004614179953932762 - train loss std (mse): 0.0007643108256161213 - val loss: 6.000982761383057 - val mse loss: 0.003373952815309167 - val loss std (mse): 0.0005087288445793092
INFO:training log:Time taken for 1 epoch: 2676.458587884903 secs

INFO:training log:Epoch 24/30
INFO:training log:final weights of first 3 elements of batch: [0.04110981 0.04017417 0.03998792 0.04089904 0.03963928 0.0398095
 0.04055951 0.03977593 0.0408767  0.03859315 0.03878266 0.04089342
 0.04088514 0.03976528 0.03910528 0.0405631  0.04084391 0.03871002
 0.03920897 0.04096978 0.04003037 0.03973013 0.03980983 0.03928496
 0.03999211], [0.03980843 0.03986764 0.04012252 0.03974121 0.03974315 0.04019539
 0.03982295 0.04015806 0.03993896 0.04006474 0.03965108 0.04006851
 0.04024716 0.04006685 0.0400419  0.04006168 0.04024065 0.04042004
 0.03951431 0.03990553 0.03985476 0.04029307 0.03988531 0.04027385
 0.04001215], [0.03962606 0.03986012 0.0396011  0.04013922 0.04040452 0.04005742
 0.04045895 0.04010225 0.04026439 0.04038703 0.04029892 0.03987304
 0.03942169 0.03957628 0.04028023 0.03975847 0.04015828 0.03965041
 0.04029541 0.03965063 0.04022433 0.04044594 0.0399396  0.03920589
 0.04031977]
INFO:training log:train loss 5.997380256652832 -  train mse loss: 0.004655149299651384 - train loss std (mse): 0.000817330030258745 - val loss: 6.006402969360352 - val mse loss: 0.003332610707730055 - val loss std (mse): 0.0004964896361343563
INFO:training log:Time taken for 1 epoch: 2676.650867462158 secs

INFO:training log:Epoch 25/30
INFO:training log:final weights of first 3 elements of batch: [0.04113184 0.04226749 0.0388733  0.04069404 0.04024439 0.03962395
 0.03950344 0.03936015 0.0393757  0.03958634 0.04022097 0.03971713
 0.03924348 0.04071805 0.03939987 0.03995118 0.0395387  0.03984845
 0.04077532 0.03966388 0.0406712  0.03912373 0.04047563 0.04018763
 0.03980421], [0.0402383  0.04006982 0.04048295 0.04011839 0.03995787 0.03986435
 0.03975063 0.03943937 0.04004985 0.04021759 0.04000408 0.03996601
 0.03992793 0.03989335 0.040087   0.03997621 0.04000981 0.03969759
 0.04053243 0.04000625 0.039859   0.03991758 0.03974056 0.04019224
 0.04000082], [0.03936921 0.03991199 0.03980963 0.04000827 0.03959328 0.03994345
 0.04021427 0.04065639 0.04038584 0.04042479 0.03948571 0.04042345
 0.04008326 0.03988655 0.04004359 0.04057932 0.0398913  0.03956212
 0.03993244 0.03969264 0.0394511  0.04010808 0.04041808 0.03983891
 0.0402863 ]
INFO:training log:train loss 6.006498336791992 -  train mse loss: 0.004954363219439983 - train loss std (mse): 0.000783354218583554 - val loss: 6.00722599029541 - val mse loss: 0.0032846515532583 - val loss std (mse): 0.0004944985848851502
INFO:training log:Time taken for 1 epoch: 2675.381111383438 secs

INFO:training log:Epoch 26/30
INFO:training log:final weights of first 3 elements of batch: [0.03980406 0.04037657 0.03919616 0.04048617 0.04065357 0.04021297
 0.04053199 0.03959714 0.03940439 0.03968848 0.04017713 0.03920804
 0.04026978 0.03990715 0.04049772 0.03999602 0.03985182 0.03988868
 0.03922717 0.04099148 0.03934615 0.04062251 0.04111587 0.039697
 0.0392521 ], [0.04012061 0.0401291  0.0402145  0.03991176 0.0399552  0.03973587
 0.04014569 0.04019861 0.04055658 0.04019014 0.03996875 0.0399261
 0.04008902 0.03980516 0.03939578 0.04017216 0.03980502 0.04018129
 0.03978461 0.03996791 0.03953613 0.04016203 0.0398371  0.04007102
 0.04013987], [0.04011374 0.04028279 0.03995086 0.03983885 0.03944295 0.04024456
 0.04029887 0.04020987 0.03963051 0.04016151 0.04049638 0.03983906
 0.0397655  0.03983182 0.03987053 0.04035829 0.04026608 0.03951241
 0.03987876 0.03915882 0.04056019 0.04021415 0.03993696 0.04008977
 0.04004683]
INFO:training log:train loss 6.006617546081543 -  train mse loss: 0.004614280536770821 - train loss std (mse): 0.0007010847912169993 - val loss: 5.999505996704102 - val mse loss: 0.00323451217263937 - val loss std (mse): 0.0004816167347598821
INFO:training log:Time taken for 1 epoch: 2672.588343858719 secs

INFO:training log:Epoch 27/30
INFO:training log:final weights of first 3 elements of batch: [0.03771365 0.03991599 0.04043    0.0397006  0.03992807 0.03926531
 0.0425834  0.04139476 0.040038   0.03904939 0.03976851 0.03872533
 0.04017643 0.03870464 0.0403139  0.03930778 0.03985954 0.03964989
 0.04087565 0.03925596 0.04031566 0.04113913 0.04123282 0.03978399
 0.04087161], [0.04018126 0.03990425 0.03995085 0.04018829 0.03997552 0.03993815
 0.03991148 0.03994606 0.04025057 0.04006473 0.04010433 0.04013004
 0.04013259 0.03965259 0.04003112 0.0398853  0.04043767 0.0399842
 0.0399768  0.03973805 0.03992382 0.04018661 0.04005758 0.03962441
 0.03982371], [0.03994477 0.0396561  0.04039266 0.03956598 0.04026914 0.03977327
 0.03987209 0.03954197 0.03955224 0.03977985 0.03972275 0.039906
 0.03990452 0.0408053  0.03978734 0.03983784 0.04009068 0.04022433
 0.04034677 0.04073695 0.04006713 0.03974467 0.03998927 0.04035047
 0.04013783]
INFO:training log:train loss 6.002847194671631 -  train mse loss: 0.004363535903394222 - train loss std (mse): 0.0006240260554477572 - val loss: 6.004910469055176 - val mse loss: 0.003201085375621915 - val loss std (mse): 0.0004618728125933558
INFO:training log:Time taken for 1 epoch: 2677.8277208805084 secs

INFO:training log:Epoch 28/30
INFO:training log:final weights of first 3 elements of batch: [0.03932242 0.03999612 0.0398098  0.03910202 0.0400161  0.04045229
 0.04094876 0.04006895 0.03918503 0.03954433 0.04045893 0.03938039
 0.04008208 0.03980827 0.04025704 0.03968653 0.04041302 0.04106339
 0.03939715 0.04052912 0.03982924 0.03903716 0.04062968 0.04068426
 0.04029798], [0.0398145  0.03987386 0.03995652 0.03980325 0.04027236 0.04049897
 0.04003861 0.03977438 0.04025484 0.03974699 0.03998711 0.03996059
 0.03983694 0.03998351 0.04008944 0.04010794 0.03962699 0.03993941
 0.03991462 0.04016202 0.0402437  0.04013194 0.04008126 0.04003033
 0.03986987], [0.03995378 0.04004345 0.04021465 0.03988488 0.04010946 0.04049059
 0.04007075 0.04045675 0.03979768 0.03956858 0.03972606 0.039691
 0.04042943 0.03982665 0.04014799 0.04063619 0.0398396  0.03926076
 0.03954134 0.04020358 0.04051188 0.03983128 0.04001847 0.03992971
 0.03981544]
INFO:training log:train loss 6.001265525817871 -  train mse loss: 0.004146135412156582 - train loss std (mse): 0.0005739257903769612 - val loss: 6.006842613220215 - val mse loss: 0.003231355920433998 - val loss std (mse): 0.0004532554012257606
INFO:training log:Time taken for 1 epoch: 2679.6184883117676 secs

INFO:training log:Epoch 29/30
INFO:training log:final weights of first 3 elements of batch: [0.03921235 0.04091628 0.03999038 0.03955225 0.04080518 0.03942686
 0.03964578 0.0390068  0.03993564 0.04027432 0.04068432 0.04023441
 0.03902769 0.0393753  0.03983919 0.04175175 0.03945614 0.04119739
 0.04059071 0.04016246 0.03990383 0.03957071 0.03908845 0.0402866
 0.04006519], [0.04011557 0.04022625 0.04012645 0.03987495 0.03979111 0.04015518
 0.04003866 0.04012574 0.03940262 0.04020048 0.04010713 0.04000328
 0.03995887 0.04020173 0.0401885  0.04006398 0.03987804 0.0397626
 0.03998856 0.03994865 0.03998061 0.04002389 0.03991698 0.03987341
 0.04004671], [0.03985521 0.04002775 0.03978655 0.04009387 0.03990036 0.04046573
 0.04005349 0.03992353 0.04019495 0.03995815 0.04004994 0.04016748
 0.04057082 0.03988116 0.04000173 0.04023387 0.03949773 0.03977446
 0.04029124 0.03930748 0.03999969 0.04004317 0.04018563 0.03979665
 0.03993933]
INFO:training log:train loss 6.002987384796143 -  train mse loss: 0.0042083533480763435 - train loss std (mse): 0.0005662693874910474 - val loss: 6.004472255706787 - val mse loss: 0.003233614144846797 - val loss std (mse): 0.0004470631538424641
INFO:training log:Time taken for 1 epoch: 2676.6285569667816 secs

INFO:training log:Epoch 30/30
INFO:training log:final weights of first 3 elements of batch: [0.03976851 0.04197337 0.03905473 0.04034736 0.04017577 0.03868077
 0.04013133 0.03979755 0.03955775 0.04059264 0.04083687 0.04033851
 0.0408357  0.03949699 0.04001343 0.03906567 0.04007183 0.03998084
 0.0399708  0.04072449 0.04000359 0.03999118 0.04003878 0.04016927
 0.03838225], [0.03978451 0.03972023 0.04001566 0.03986771 0.04009932 0.03985844
 0.03949565 0.03996105 0.04013896 0.04036132 0.03996433 0.04000755
 0.04011423 0.04006287 0.03991926 0.04039166 0.03981299 0.04037267
 0.04013221 0.04030159 0.03999192 0.04000519 0.03981417 0.03995716
 0.03984937], [0.04008981 0.04006176 0.04026136 0.03977238 0.0393512  0.03969118
 0.0395919  0.04027714 0.04003424 0.04033606 0.04026072 0.04017011
 0.04019979 0.03987996 0.04010302 0.04002437 0.04028616 0.03989045
 0.04021452 0.03999037 0.0401644  0.03936336 0.03988331 0.04023962
 0.03986288]
INFO:training log:train loss 6.003661632537842 -  train mse loss: 0.004158038645982742 - train loss std (mse): 0.0005668009398505092 - val loss: 6.006102085113525 - val mse loss: 0.0031353312078863382 - val loss std (mse): 0.0004292160156182945
INFO:training log:Time taken for 1 epoch: 2676.695320367813 secs

INFO:training log:total training time for 30 epochs:26795.804544448853
INFO:training log:saving loss and metrics information...
INFO:training log:saving model output in .npy files...
INFO:training log:training of SMC Transformer for a time-series dataset done...
INFO:training log:>>>--------------------------------------------------------------------------------------------------------------------------------------------------------------<<<
