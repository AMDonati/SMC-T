INFO:training log:model hyperparameters from the config file: {'num_layers': 1, 'num_heads': 4, 'd_model': 12, 'dff': 48, 'rate': 0.1, 'maximum_position_encoding_baseline': 50, 'maximum_position_encoding_smc': 'None'}
INFO:training log:smc hyperparameters from the config file: {'num_particles': 25, 'noise_encoder': 'False', 'noise_SMC_layer': 'True', 'sigma': 0.05}
INFO:training log:starting the training of the smc transformer...
INFO:training log:number of training samples: 336290
INFO:training log:steps per epoch: 320
WARNING:tensorflow:Layer smc__transformer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

INFO:training log:Epoch 1/30
INFO:training log:model hyperparameters from the config file: {'num_layers': 1, 'num_heads': 4, 'd_model': 12, 'dff': 48, 'rate': 0.1, 'maximum_position_encoding_baseline': 50, 'maximum_position_encoding_smc': 'None'}
INFO:training log:smc hyperparameters from the config file: {'num_particles': 25, 'noise_encoder': 'False', 'noise_SMC_layer': 'True', 'sigma': 0.05}
INFO:training log:starting the training of the smc transformer...
INFO:training log:number of training samples: 336290
INFO:training log:steps per epoch: 320
WARNING:tensorflow:Layer smc__transformer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

INFO:training log:Epoch 1/30
INFO:training log:final weights of first 3 elements of batch: [0.03490741 0.03765296 0.04013442 0.04027828 0.03959396 0.04176029
 0.04060353 0.03985173 0.04106045 0.04034363 0.0381516  0.04276161
 0.04126924 0.04142357 0.03915042 0.04026202 0.04123697 0.04223296
 0.04028208 0.03563871 0.04157104 0.04074535 0.0375706  0.0407289
 0.04078819], [0.04039316 0.04104246 0.0411786  0.03994732 0.04028999 0.04114527
 0.03927087 0.04113235 0.04035425 0.03885661 0.03715391 0.04055305
 0.03678264 0.03934576 0.04034451 0.04077761 0.03968745 0.04111673
 0.03931737 0.04030718 0.04116992 0.03872247 0.0391533  0.0411449
 0.04081232], [0.04192492 0.04374595 0.03710015 0.04283423 0.03964591 0.03448208
 0.0440464  0.04311418 0.04318311 0.0395112  0.03753702 0.03258992
 0.03922234 0.04094658 0.03909524 0.03767659 0.03967223 0.04433828
 0.03493564 0.04116379 0.04054947 0.04137867 0.03839963 0.03763948
 0.04526703]
INFO:training log:train loss 24.296476364135742 -  train mse loss: 0.29416796565055847 - train loss std (mse): 0.04411165043711662 - val loss: 24.204872131347656 - val mse loss: 0.2049897462129593 - val loss std (mse): 0.016179969534277916
INFO:training log:Time taken for 1 epoch: 3816.1536729335785 secs

INFO:training log:Epoch 2/30
INFO:training log:final weights of first 3 elements of batch: [0.03903165 0.04050944 0.04028038 0.0404995  0.03942255 0.04012964
 0.04036538 0.03995075 0.04022432 0.03849042 0.03942368 0.03997984
 0.04031989 0.03984369 0.04013377 0.04007553 0.04042973 0.04017131
 0.0403809  0.04039356 0.03931691 0.03952814 0.04030853 0.04048218
 0.04030829], [0.03995256 0.04015785 0.04017946 0.04007969 0.03972585 0.04015468
 0.04010947 0.0397254  0.04013962 0.040191   0.03939431 0.04009654
 0.03951254 0.0401909  0.04018965 0.04010287 0.04018521 0.04018367
 0.04016635 0.04019101 0.04019039 0.03937041 0.03970958 0.04016259
 0.03993845], [0.04142782 0.03989312 0.04298966 0.04233205 0.04202458 0.04113225
 0.03989714 0.03841964 0.03644875 0.04105363 0.03945132 0.04139796
 0.04095108 0.04121201 0.03942594 0.03939914 0.04174393 0.03863236
 0.03994035 0.0418826  0.04264184 0.03521003 0.04001514 0.03930942
 0.03316824]
INFO:training log:train loss 24.045068740844727 -  train mse loss: 0.05354354903101921 - train loss std (mse): 0.013240993954241276 - val loss: 24.019126892089844 - val mse loss: 0.027942275628447533 - val loss std (mse): 0.0042286217212677
INFO:training log:Time taken for 1 epoch: 3841.3348019123077 secs

INFO:training log:Epoch 3/30
INFO:training log:final weights of first 3 elements of batch: [0.03982251 0.03933169 0.04008757 0.04023419 0.03994219 0.04016214
 0.04022422 0.04017453 0.03984525 0.04010828 0.03971609 0.04006623
 0.040294   0.03982414 0.03987424 0.04009721 0.04018392 0.03986743
 0.04001852 0.04008136 0.04008468 0.03956454 0.04029259 0.04022939
 0.03987304], [0.04008799 0.04010208 0.04010253 0.04000369 0.0398022  0.04010174
 0.04002818 0.04008412 0.04010177 0.04010178 0.04009961 0.0399867
 0.03991145 0.03997509 0.04005963 0.04003175 0.0393998  0.03992385
 0.03978007 0.04008003 0.04003461 0.04003131 0.04009483 0.0400705
 0.04000468], [0.03626062 0.03947151 0.04046271 0.03924669 0.04263323 0.04014526
 0.03930862 0.04134249 0.04043191 0.03983245 0.03847544 0.03854038
 0.03974276 0.04071103 0.03871608 0.04028723 0.04053769 0.0411906
 0.04068338 0.04040067 0.03993022 0.03940694 0.03960945 0.04144685
 0.04118574]
INFO:training log:train loss 24.028539657592773 -  train mse loss: 0.019919201731681824 - train loss std (mse): 0.004815884865820408 - val loss: 24.000762939453125 - val mse loss: 0.010813557542860508 - val loss std (mse): 0.0019003673223778605
INFO:training log:Time taken for 1 epoch: 3836.9618051052094 secs

INFO:training log:Epoch 4/30
INFO:training log:final weights of first 3 elements of batch: [0.0398399  0.04022832 0.04011336 0.04019238 0.03974479 0.04015547
 0.03985033 0.04002887 0.04012804 0.04032202 0.04030406 0.03967594
 0.03970109 0.04006815 0.04025195 0.04022593 0.03973785 0.04008684
 0.04022688 0.04021363 0.03984654 0.04025633 0.03978257 0.03922249
 0.03979633], [0.04011204 0.03998376 0.04001483 0.04006778 0.0399069  0.0401024
 0.03998625 0.03999139 0.04002061 0.04012474 0.04010276 0.03991776
 0.04008368 0.03975074 0.03998829 0.04008656 0.03984988 0.03979922
 0.03976545 0.04006847 0.04012485 0.04007721 0.04011954 0.03996279
 0.03999205], [0.04077104 0.04065922 0.03997645 0.04005508 0.03861785 0.03921374
 0.03915437 0.03961882 0.04038902 0.04074465 0.04009327 0.03917338
 0.0412492  0.04218882 0.03891915 0.04003046 0.03830824 0.04129551
 0.0401244  0.03798185 0.04108588 0.0410524  0.04035513 0.03924747
 0.03969458]
INFO:training log:train loss 24.0159969329834 -  train mse loss: 0.010725483298301697 - train loss std (mse): 0.0026723055634647608 - val loss: 24.00067138671875 - val mse loss: 0.006322021130472422 - val loss std (mse): 0.0012845381861552596
INFO:training log:Time taken for 1 epoch: 3837.339437007904 secs

INFO:training log:Epoch 5/30
INFO:training log:final weights of first 3 elements of batch: [0.04034952 0.04010665 0.0397184  0.03981216 0.04018559 0.039828
 0.04019898 0.04008546 0.03998916 0.04022439 0.03992435 0.03984779
 0.03993599 0.03996506 0.03986256 0.03987356 0.0399366  0.03994081
 0.04003327 0.03960671 0.04003735 0.04000579 0.04029886 0.03996746
 0.04026553], [0.0400482  0.04004779 0.04004357 0.04004483 0.03998887 0.04004661
 0.04003287 0.0400461  0.03993696 0.04002618 0.04000418 0.03979714
 0.03994041 0.04004328 0.04000133 0.03994542 0.03980495 0.04002873
 0.0400497  0.04003349 0.04005033 0.0400163  0.03999144 0.03998128
 0.04005007], [0.04060017 0.03809245 0.04027349 0.040949   0.03975358 0.04075836
 0.04104595 0.03983787 0.03838647 0.03947435 0.03965433 0.04060612
 0.04056039 0.04107441 0.03931231 0.0394648  0.039251   0.039344
 0.04048056 0.03874223 0.04223651 0.04123481 0.03875668 0.04032054
 0.03978964]
INFO:training log:train loss 24.002405166625977 -  train mse loss: 0.007948407903313637 - train loss std (mse): 0.0018669981509447098 - val loss: 24.006376266479492 - val mse loss: 0.004897791892290115 - val loss std (mse): 0.0010147663997486234
INFO:training log:Time taken for 1 epoch: 3837.8952112197876 secs

INFO:training log:Epoch 6/30
INFO:training log:final weights of first 3 elements of batch: [0.03994785 0.03966394 0.04022577 0.04020603 0.03998957 0.0399529
 0.04007117 0.03964991 0.0400816  0.0399892  0.04014286 0.0396447
 0.03987365 0.04011209 0.03979605 0.03949328 0.03972764 0.04015809
 0.0400692  0.04027195 0.04036109 0.04026602 0.04009477 0.04026736
 0.03994328], [0.03995284 0.0399605  0.04001022 0.04003887 0.0400201  0.03994279
 0.0400155  0.03999973 0.04003764 0.04002887 0.04003176 0.04003878
 0.04002722 0.04002588 0.04003888 0.04002124 0.04003757 0.03986682
 0.03982891 0.04001752 0.04003245 0.0400317  0.04001182 0.03994348
 0.04003889], [0.04039305 0.03919052 0.03991625 0.04116327 0.04028593 0.04046459
 0.04065701 0.04172016 0.04086444 0.0395829  0.03917411 0.03915397
 0.04056142 0.0399319  0.03878269 0.0383857  0.04122403 0.03891864
 0.04028477 0.03908511 0.0417243  0.03920837 0.03945756 0.03924359
 0.04062569]
INFO:training log:train loss 24.015790939331055 -  train mse loss: 0.006460100412368774 - train loss std (mse): 0.0015419702976942062 - val loss: 24.00600814819336 - val mse loss: 0.0040378631092607975 - val loss std (mse): 0.0008367321570403874
INFO:training log:Time taken for 1 epoch: 3841.1364693641663 secs

INFO:training log:Epoch 7/30
INFO:training log:final weights of first 3 elements of batch: [0.03984775 0.04027641 0.04020328 0.03955092 0.04006074 0.03993694
 0.03980648 0.04027868 0.04002676 0.0396725  0.039751   0.03981223
 0.03997569 0.04021198 0.04016158 0.04007962 0.04004205 0.03979452
 0.04013941 0.04022902 0.04006383 0.04026183 0.04027342 0.03993092
 0.0396124 ], [0.03999909 0.04001784 0.03993826 0.04001763 0.03999972 0.04001734
 0.03998972 0.03997061 0.04000424 0.03995756 0.0400172  0.03994673
 0.04001778 0.04001549 0.04000442 0.0399982  0.04001785 0.04001503
 0.04001633 0.03999746 0.04001773 0.04001756 0.03998104 0.04001765
 0.04000746], [0.04030522 0.04013653 0.03944789 0.04160741 0.04056748 0.04026172
 0.0398665  0.04118812 0.03989075 0.04106013 0.04018274 0.03866976
 0.039568   0.04036197 0.03953154 0.04017594 0.04017245 0.03917215
 0.03950251 0.04006407 0.03944809 0.03863164 0.04029156 0.0391917
 0.04070414]
INFO:training log:train loss 24.00762176513672 -  train mse loss: 0.005747376009821892 - train loss std (mse): 0.0012476634001359344 - val loss: 24.010543823242188 - val mse loss: 0.003704802831634879 - val loss std (mse): 0.000741500873118639
INFO:training log:Time taken for 1 epoch: 3838.441948413849 secs

INFO:training log:Epoch 8/30
INFO:training log:final weights of first 3 elements of batch: [0.03990332 0.0400153  0.03982102 0.0397516  0.03985627 0.04025801
 0.0398873  0.03987581 0.03987743 0.04006685 0.03999453 0.0398975
 0.04001347 0.03981994 0.03984417 0.04011025 0.04031341 0.03990281
 0.04029913 0.04003887 0.03997954 0.04015949 0.03992051 0.04019688
 0.04019662], [0.03999741 0.03997868 0.04000528 0.04001725 0.03999859 0.03999674
 0.04002328 0.0400103  0.04002649 0.04002201 0.03996281 0.04002099
 0.04002661 0.04002292 0.04001812 0.03998018 0.04002661 0.04001772
 0.03997607 0.03994636 0.04000334 0.03993891 0.04001385 0.04002528
 0.03994423], [0.04078845 0.04057536 0.03987265 0.03819294 0.03833614 0.04053778
 0.03905749 0.04104292 0.03979219 0.03970994 0.03927551 0.0399494
 0.04008586 0.04081138 0.04067884 0.03955392 0.04005944 0.03951667
 0.04107039 0.03997901 0.03983672 0.04009406 0.04085016 0.04005077
 0.04028206]
INFO:training log:train loss 24.015413284301758 -  train mse loss: 0.005066981073468924 - train loss std (mse): 0.0010661830892786384 - val loss: 23.998525619506836 - val mse loss: 0.003490931587293744 - val loss std (mse): 0.0006816142704337835
INFO:training log:Time taken for 1 epoch: 3845.2005977630615 secs

INFO:training log:Epoch 9/30
INFO:training log:final weights of first 3 elements of batch: [0.04034354 0.03981743 0.03980259 0.03988157 0.0401022  0.04000337
 0.04049395 0.04011003 0.03994844 0.04031358 0.03989867 0.0396467
 0.04008147 0.04003472 0.04006493 0.03979302 0.04026689 0.03999871
 0.03983028 0.03993576 0.04007754 0.03984348 0.04008206 0.03976402
 0.03986503], [0.04001183 0.03989286 0.04001271 0.03999821 0.04001162 0.04000863
 0.03998668 0.04000923 0.0400138  0.04001046 0.04000397 0.04000078
 0.03999691 0.04000933 0.04001101 0.04000971 0.03998615 0.03997001
 0.04000971 0.04001129 0.04000567 0.04000983 0.04001134 0.04000314
 0.04000511], [0.04082104 0.03888377 0.04144974 0.03957793 0.04051641 0.04003287
 0.04077519 0.04097062 0.03969087 0.03948439 0.04031879 0.03965334
 0.03879356 0.04099758 0.03943253 0.04050833 0.04005845 0.03970589
 0.03975629 0.03958631 0.04032731 0.0397805  0.03996583 0.03936604
 0.03954643]
INFO:training log:train loss 23.99953842163086 -  train mse loss: 0.004540904890745878 - train loss std (mse): 0.0009658001363277435 - val loss: 24.00459098815918 - val mse loss: 0.0033894276712089777 - val loss std (mse): 0.0006310524186119437
INFO:training log:Time taken for 1 epoch: 3840.60418176651 secs

INFO:training log:Epoch 10/30
INFO:training log:final weights of first 3 elements of batch: [0.03995138 0.0400011  0.04009956 0.04021093 0.0401634  0.0400798
 0.0398693  0.04000857 0.04011556 0.03997612 0.03982712 0.03978359
 0.04000948 0.04017928 0.04001521 0.04005726 0.0400755  0.0396563
 0.04014669 0.04004324 0.03976276 0.04031902 0.03985697 0.03991424
 0.03987763], [0.03998659 0.04001226 0.03999425 0.04001348 0.03996985 0.03999006
 0.04000024 0.04000891 0.03999365 0.03998573 0.04001223 0.04001317
 0.03999029 0.04000003 0.03999263 0.04001181 0.04000945 0.04001341
 0.0400111  0.04001135 0.03996459 0.03997992 0.04001346 0.04001226
 0.04000925], [0.0406861  0.0402728  0.03996647 0.0405802  0.03930297 0.03969585
 0.04007624 0.03993312 0.04044749 0.03976826 0.03982409 0.03997766
 0.04060562 0.03981393 0.03934123 0.04066019 0.03899225 0.04075871
 0.0387821  0.04008069 0.04021635 0.04001861 0.03984891 0.04048374
 0.03986645]
INFO:training log:train loss 24.004497528076172 -  train mse loss: 0.005244377069175243 - train loss std (mse): 0.0010543178068473935 - val loss: 23.999860763549805 - val mse loss: 0.0030679197516292334 - val loss std (mse): 0.0005850352463312447
INFO:training log:Time taken for 1 epoch: 3843.2843952178955 secs

INFO:training log:Epoch 11/30
INFO:training log:final weights of first 3 elements of batch: [0.03976088 0.03999322 0.0403242  0.04003588 0.04019636 0.03994618
 0.03997995 0.0395602  0.04003919 0.04012354 0.04011995 0.04013544
 0.04004598 0.0401846  0.03992329 0.03999051 0.040016   0.0400757
 0.04011844 0.03998041 0.03958581 0.03998451 0.04011159 0.03973747
 0.04003074], [0.03998915 0.04000641 0.04002044 0.03999272 0.04001927 0.04000041
 0.04001728 0.03996044 0.04000629 0.0400128  0.04001044 0.0400193
 0.03979409 0.04001384 0.0400209  0.04000131 0.04001259 0.04001177
 0.0399982  0.04000428 0.040021   0.04000844 0.04001929 0.04001832
 0.04002104], [0.0393877  0.03952297 0.04084848 0.03952822 0.04024211 0.04003525
 0.04003623 0.04054852 0.04038195 0.03991762 0.03925585 0.04020464
 0.04042934 0.03984243 0.04002224 0.03998886 0.04010384 0.03944276
 0.04037131 0.04027946 0.04024045 0.03891639 0.04050617 0.04003044
 0.03991676]
INFO:training log:train loss 24.000017166137695 -  train mse loss: 0.00451443437486887 - train loss std (mse): 0.0009264082764275372 - val loss: 24.003774642944336 - val mse loss: 0.0030705404933542013 - val loss std (mse): 0.000544606416951865
INFO:training log:Time taken for 1 epoch: 3857.4202020168304 secs

INFO:training log:Epoch 12/30
INFO:training log:final weights of first 3 elements of batch: [0.03985431 0.04009843 0.03996234 0.03996487 0.03984017 0.04019176
 0.04014608 0.04017655 0.0398427  0.03985561 0.04004394 0.04019275
 0.04005168 0.04007796 0.03983595 0.03982758 0.03994642 0.04009487
 0.04004301 0.04020549 0.04020992 0.03984065 0.03960273 0.04010343
 0.03999075], [0.0399147  0.03992871 0.04002989 0.03997374 0.04002941 0.04002892
 0.04000557 0.03996604 0.03992896 0.04002188 0.04002985 0.04002725
 0.04001112 0.03998252 0.04002779 0.04001179 0.04000085 0.04001789
 0.04001036 0.04002967 0.03997709 0.04002958 0.03999003 0.04000337
 0.04002305], [0.04015463 0.04046642 0.03906295 0.0402631  0.03963106 0.04028917
 0.03947537 0.03964341 0.03971896 0.04026284 0.040652   0.03908422
 0.03939329 0.03980238 0.04004756 0.03945736 0.04074327 0.03981473
 0.03943488 0.04055296 0.03979599 0.04012888 0.04048638 0.04031892
 0.0413192 ]
INFO:training log:train loss 24.004037857055664 -  train mse loss: 0.004835272673517466 - train loss std (mse): 0.0009058114374056458 - val loss: 24.007801055908203 - val mse loss: 0.0034739908296614885 - val loss std (mse): 0.0005651084356941283
INFO:training log:Time taken for 1 epoch: 3841.004528284073 secs

INFO:training log:Epoch 13/30
INFO:training log:final weights of first 3 elements of batch: [0.03973031 0.04010873 0.04003622 0.0401119  0.03959851 0.04001455
 0.03981575 0.03987537 0.04018152 0.04000016 0.04000655 0.0402884
 0.04002689 0.04032957 0.04008646 0.04007405 0.03991595 0.03979416
 0.03986632 0.0399368  0.03999282 0.04018943 0.0401165  0.03983633
 0.0400667 ], [0.03991348 0.03999124 0.04004824 0.04004808 0.04003881 0.03999913
 0.0400006  0.04000773 0.03994749 0.03996734 0.03995003 0.03996641
 0.04002166 0.04000725 0.04001854 0.04002815 0.04004596 0.04001759
 0.04001726 0.03999165 0.03997109 0.03997646 0.03996218 0.040047
 0.0400166 ], [0.04010925 0.04042127 0.03992001 0.04006257 0.03982966 0.03919889
 0.0410309  0.04069297 0.04031968 0.03979539 0.03894491 0.04038262
 0.03964529 0.04043433 0.03956728 0.0401808  0.04051972 0.03963793
 0.04026796 0.04006425 0.03953736 0.0391621  0.03993991 0.04019634
 0.04013859]
INFO:training log:train loss 24.00081443786621 -  train mse loss: 0.0042165545746684074 - train loss std (mse): 0.0007727694464847445 - val loss: 24.002403259277344 - val mse loss: 0.0032503786496818066 - val loss std (mse): 0.0005243255291134119
INFO:training log:Time taken for 1 epoch: 3855.5997858047485 secs

INFO:training log:Epoch 14/30
INFO:training log:final weights of first 3 elements of batch: [0.03982482 0.04016307 0.0403316  0.03977922 0.03978534 0.03998968
 0.0400552  0.04000056 0.03976408 0.04021118 0.04029458 0.03991735
 0.04004994 0.03984106 0.04021059 0.04022337 0.04005513 0.03986416
 0.03984401 0.03987275 0.03989315 0.04021843 0.04001979 0.03961359
 0.04017731], [0.04001544 0.03999901 0.03999742 0.03992356 0.04000004 0.04002435
 0.04001826 0.04001996 0.03999801 0.03998156 0.04002484 0.04000594
 0.03991727 0.04001439 0.04002501 0.03999244 0.04002493 0.04002503
 0.0399201  0.04002433 0.0400243  0.04001011 0.04000313 0.04002377
 0.03998679], [0.04010965 0.03984923 0.0403059  0.03956648 0.03978892 0.0398174
 0.04000294 0.04059305 0.03840894 0.04069918 0.04018074 0.03993532
 0.04043931 0.03922885 0.04080065 0.04003147 0.0399593  0.0394959
 0.04000179 0.04000955 0.04010305 0.04009161 0.03970378 0.04072843
 0.04014858]
INFO:training log:train loss 24.006315231323242 -  train mse loss: 0.003769833594560623 - train loss std (mse): 0.0007028373656794429 - val loss: 23.999555587768555 - val mse loss: 0.002657666802406311 - val loss std (mse): 0.0004544379189610481
INFO:training log:Time taken for 1 epoch: 3845.249897956848 secs

INFO:training log:Epoch 15/30
INFO:training log:final weights of first 3 elements of batch: [0.03985188 0.03978582 0.03963108 0.03997977 0.04018175 0.03990101
 0.0401427  0.03994728 0.03986278 0.04027003 0.03987269 0.04017863
 0.0401386  0.04008839 0.04011222 0.0401106  0.04007359 0.04003737
 0.0401299  0.03990216 0.0400966  0.04017548 0.03979721 0.03989536
 0.03983707], [0.04000396 0.03999926 0.0399573  0.04000708 0.0400236  0.04000922
 0.03993454 0.04000786 0.04000856 0.04001484 0.03994502 0.04001025
 0.04002302 0.03994992 0.04002244 0.04000646 0.03996663 0.04001036
 0.04002035 0.04002183 0.04002368 0.03999515 0.04000388 0.04002066
 0.04001418], [0.03979054 0.04121169 0.03961585 0.04055613 0.03956794 0.03915428
 0.03963343 0.04043142 0.04022387 0.04025828 0.04023389 0.03934553
 0.03967948 0.04025404 0.03971301 0.03986829 0.04048552 0.0395946
 0.04075158 0.03970863 0.03959848 0.04000511 0.04009418 0.04015987
 0.04006435]
INFO:training log:train loss 23.999433517456055 -  train mse loss: 0.003578862641006708 - train loss std (mse): 0.0006512788240797818 - val loss: 24.003637313842773 - val mse loss: 0.0026806911919265985 - val loss std (mse): 0.00044784886995330453
INFO:training log:Time taken for 1 epoch: 3881.249737262726 secs

INFO:training log:Epoch 16/30
INFO:training log:final weights of first 3 elements of batch: [0.04004544 0.04018851 0.03994225 0.03962098 0.04009208 0.039814
 0.0401158  0.0400222  0.04000999 0.04021133 0.04007353 0.03966726
 0.04012142 0.04002201 0.03988534 0.04018869 0.0401656  0.04010507
 0.03997381 0.03997025 0.03981394 0.03993724 0.03985092 0.04003512
 0.04012714], [0.04001497 0.04000304 0.04001135 0.03995988 0.04000822 0.04000757
 0.0400082  0.04001474 0.03997615 0.04001338 0.03989575 0.04001826
 0.04001176 0.03999655 0.04000339 0.03996622 0.04001374 0.04000745
 0.03999403 0.04000276 0.04001568 0.04001682 0.04001538 0.04001585
 0.0400089 ], [0.04056219 0.04061966 0.04112509 0.03975566 0.04022149 0.03966299
 0.03973783 0.03985535 0.03998842 0.03939968 0.03998229 0.03993449
 0.0395564  0.03980184 0.03964185 0.04089497 0.04096414 0.03925472
 0.039676   0.03921618 0.04011888 0.0409543  0.0397233  0.03993409
 0.03941817]
INFO:training log:train loss 23.998676300048828 -  train mse loss: 0.003724433481693268 - train loss std (mse): 0.0006727059953846037 - val loss: 24.01128387451172 - val mse loss: 0.002535042352974415 - val loss std (mse): 0.00042508007027208805
INFO:training log:Time taken for 1 epoch: 3881.6615567207336 secs

INFO:training log:Epoch 17/30
INFO:training log:final weights of first 3 elements of batch: [0.03987012 0.04007977 0.03988238 0.04009996 0.04016346 0.04000333
 0.03973603 0.03988982 0.04013024 0.04007632 0.04004039 0.04005734
 0.03986384 0.03997568 0.03987901 0.04013393 0.03997181 0.03984822
 0.0399871  0.03995889 0.04019969 0.0400314  0.03995679 0.04009885
 0.0400656 ], [0.04003926 0.03996186 0.04001242 0.03998524 0.04000903 0.03999016
 0.0400207  0.03994491 0.04003429 0.03999377 0.04002001 0.04000002
 0.03994552 0.04001572 0.04002563 0.04002919 0.04002477 0.03999487
 0.04001877 0.0399422  0.04000863 0.03999804 0.04003067 0.04000641
 0.039948  ], [0.03972025 0.04069826 0.04029423 0.03973557 0.04068085 0.04053969
 0.0395749  0.04031876 0.0397036  0.04019755 0.04022008 0.03952694
 0.03952117 0.04038207 0.03974328 0.04042659 0.04005569 0.0400583
 0.03964219 0.04025223 0.03965792 0.03972218 0.03968465 0.03985938
 0.03978362]
INFO:training log:train loss 24.00737762451172 -  train mse loss: 0.0036141658201813698 - train loss std (mse): 0.0006620794883929193 - val loss: 24.005836486816406 - val mse loss: 0.0025897559244185686 - val loss std (mse): 0.0004311230732128024
INFO:training log:Time taken for 1 epoch: 3862.564436197281 secs

INFO:training log:Epoch 18/30
INFO:training log:final weights of first 3 elements of batch: [0.03988444 0.04010588 0.04002522 0.03981755 0.03987407 0.0401622
 0.03998913 0.0399941  0.04007022 0.0400644  0.04007605 0.04015078
 0.03987261 0.03994398 0.04005644 0.03995285 0.03992852 0.0400077
 0.0399822  0.04002039 0.04001988 0.0400473  0.03993969 0.03999379
 0.04002064], [0.04001592 0.03999649 0.03999627 0.04001622 0.04001611 0.03993781
 0.04001617 0.03999167 0.0400161  0.04000546 0.03999604 0.04000817
 0.04000417 0.03995429 0.03996374 0.0400092  0.04001177 0.04001623
 0.04001074 0.04000454 0.03998244 0.04001613 0.04000565 0.04000341
 0.04000521], [0.03985088 0.03999714 0.04013598 0.03996433 0.03957302 0.04044307
 0.04047683 0.04038703 0.03991769 0.04032149 0.04032693 0.03992079
 0.04011483 0.03987654 0.04008232 0.040349   0.03948539 0.03958723
 0.03985764 0.04054941 0.03915158 0.03961772 0.04032879 0.03951042
 0.04017385]
INFO:training log:train loss 24.003320693969727 -  train mse loss: 0.0035491061862558126 - train loss std (mse): 0.000647369830403477 - val loss: 24.014427185058594 - val mse loss: 0.002426059218123555 - val loss std (mse): 0.00040380714926868677
INFO:training log:Time taken for 1 epoch: 3848.718649148941 secs

INFO:training log:Epoch 19/30
INFO:training log:final weights of first 3 elements of batch: [0.04004863 0.04007994 0.04012076 0.03993022 0.03994392 0.03994486
 0.03997755 0.04011718 0.04006708 0.03996135 0.03990713 0.04017529
 0.0398716  0.04007985 0.04007138 0.04004255 0.0398803  0.04009461
 0.03989611 0.03992835 0.04003264 0.04009168 0.04002258 0.03973261
 0.03998186], [0.04000458 0.0400001  0.04000739 0.04000964 0.0400059  0.0400101
 0.03998195 0.03999721 0.04000672 0.04000058 0.04000624 0.0399674
 0.03998058 0.03999953 0.04001033 0.04000979 0.0399782  0.04000529
 0.0400103  0.03999097 0.04001035 0.04000467 0.04000978 0.04001038
 0.03998207], [0.03921227 0.04002293 0.03974852 0.03972868 0.0402142  0.04025978
 0.03954591 0.03959227 0.03998145 0.03991974 0.03992486 0.03897377
 0.04087316 0.03992948 0.04036975 0.04037167 0.04070947 0.0405039
 0.04040473 0.04045879 0.04040636 0.03965342 0.03893129 0.04055323
 0.03971041]
INFO:training log:train loss 23.99831771850586 -  train mse loss: 0.003416320774704218 - train loss std (mse): 0.0005965034360997379 - val loss: 23.993745803833008 - val mse loss: 0.0023516840301454067 - val loss std (mse): 0.0003885322657879442
INFO:training log:Time taken for 1 epoch: 3844.212813615799 secs

INFO:training log:Epoch 20/30
INFO:training log:final weights of first 3 elements of batch: [0.04001174 0.03983233 0.039967   0.04007553 0.04012036 0.03987815
 0.03984671 0.03996966 0.04007608 0.0400819  0.04021951 0.04007554
 0.04010967 0.04006702 0.04005706 0.04004612 0.03991348 0.04003775
 0.03998503 0.03989989 0.0398362  0.04014866 0.04000274 0.03983198
 0.0399099 ], [0.03997587 0.03998324 0.0400215  0.0400226  0.04002542 0.04002177
 0.04000523 0.03999158 0.04002495 0.03998528 0.04001453 0.03999597
 0.04000339 0.03997988 0.03993494 0.04002041 0.04001    0.04001842
 0.03998764 0.03999598 0.04001679 0.04000208 0.04001969 0.03991752
 0.04002535], [0.03963343 0.04012343 0.04018295 0.04044267 0.03923026 0.04057074
 0.03916827 0.04001741 0.03980078 0.0394007  0.03932765 0.04014668
 0.0399069  0.04042442 0.04014004 0.03980702 0.04042244 0.04081181
 0.04048283 0.0397379  0.03986342 0.04044048 0.040991   0.03943387
 0.03949287]
INFO:training log:train loss 24.003812789916992 -  train mse loss: 0.0031979542691260576 - train loss std (mse): 0.0005651510437019169 - val loss: 24.010421752929688 - val mse loss: 0.0023637532722204924 - val loss std (mse): 0.0003850797365885228
INFO:training log:Time taken for 1 epoch: 3848.9531037807465 secs

INFO:training log:Epoch 21/30
INFO:training log:final weights of first 3 elements of batch: [0.04007659 0.03998091 0.03989726 0.03999421 0.04012718 0.03995512
 0.04004898 0.03995851 0.03995199 0.04001068 0.03999304 0.03977946
 0.03998065 0.03996466 0.04029737 0.04006907 0.04008001 0.03994944
 0.03977577 0.04017723 0.04001454 0.03993343 0.03999315 0.0401468
 0.03984399], [0.04000995 0.03999731 0.03998953 0.03998204 0.04001021 0.04000952
 0.04000993 0.04000839 0.04001    0.03998312 0.04001027 0.04000704
 0.04000456 0.0399862  0.04000561 0.03999862 0.04000149 0.04001027
 0.04000498 0.03994736 0.0399781  0.04000872 0.04000945 0.04000761
 0.04000971], [0.03970946 0.04049707 0.03951879 0.04039604 0.03953131 0.04016075
 0.04003253 0.04058388 0.04050436 0.03991429 0.04025965 0.03991711
 0.03992137 0.03960325 0.04063446 0.04084284 0.0400438  0.03970427
 0.04071391 0.03974529 0.03937935 0.03980384 0.03945925 0.03919553
 0.03992756]
INFO:training log:train loss 23.99734115600586 -  train mse loss: 0.003352491185069084 - train loss std (mse): 0.0005782072548754513 - val loss: 24.015567779541016 - val mse loss: 0.002426605671644211 - val loss std (mse): 0.0003764272260013968
INFO:training log:Time taken for 1 epoch: 3840.652287006378 secs

INFO:training log:Epoch 22/30
INFO:training log:final weights of first 3 elements of batch: [0.0400039  0.04005595 0.0400414  0.03984113 0.04019259 0.03984583
 0.03982241 0.03994046 0.04000191 0.04005014 0.04019033 0.0399749
 0.04005501 0.04007043 0.04016228 0.03994675 0.03990889 0.03988204
 0.03995036 0.04007626 0.03995992 0.04009618 0.04004811 0.03975619
 0.04012665], [0.04000163 0.03998864 0.04001191 0.03994458 0.03999485 0.0400105
 0.04002175 0.0400209  0.04002032 0.0400067  0.0400184  0.0400056
 0.04001452 0.04001534 0.03999282 0.03997424 0.03998266 0.04002078
 0.04001203 0.04002056 0.04002096 0.03998088 0.04002032 0.03997744
 0.03992168], [0.04005506 0.03999592 0.04024327 0.04088628 0.03960188 0.04023934
 0.03970886 0.04003063 0.04031195 0.03925843 0.03989735 0.03987554
 0.03978957 0.04028128 0.04014252 0.04007238 0.03966044 0.04004182
 0.03972109 0.04011066 0.03975782 0.03986399 0.03972874 0.04060492
 0.04012033]
INFO:training log:train loss 24.00103759765625 -  train mse loss: 0.003417129395529628 - train loss std (mse): 0.0005823328392580152 - val loss: 24.010059356689453 - val mse loss: 0.0022503165528178215 - val loss std (mse): 0.00035738485166803
INFO:training log:Time taken for 1 epoch: 3847.0012726783752 secs

INFO:training log:Epoch 23/30
INFO:training log:final weights of first 3 elements of batch: [0.04001831 0.0400052  0.04003672 0.03995796 0.04001557 0.0399192
 0.04003745 0.04006838 0.03995899 0.04000865 0.04004228 0.0399307
 0.03994343 0.03989204 0.03997599 0.0401067  0.03998032 0.03985114
 0.04003293 0.04018601 0.03991243 0.04004852 0.0400499  0.03996344
 0.04005774], [0.0399501  0.0400034  0.03995694 0.04001264 0.04002339 0.04000442
 0.04000333 0.03998302 0.04002213 0.03987708 0.03998188 0.04002191
 0.04000741 0.04002339 0.04002162 0.04000317 0.04002198 0.039993
 0.04002081 0.04002286 0.0400226  0.04001823 0.04000096 0.03999621
 0.04000749], [0.0399001  0.04046186 0.03999522 0.04010293 0.04019319 0.03924697
 0.04011359 0.04009804 0.0408028  0.04035841 0.04048051 0.03977301
 0.04005856 0.03942667 0.03970576 0.0407266  0.03987399 0.04007746
 0.03997109 0.03957943 0.03998204 0.03954556 0.04001341 0.03961236
 0.03990052]
INFO:training log:train loss 24.007478713989258 -  train mse loss: 0.0032273782417178154 - train loss std (mse): 0.0005585125763900578 - val loss: 23.989574432373047 - val mse loss: 0.0022918754257261753 - val loss std (mse): 0.00035818666219711304
INFO:training log:Time taken for 1 epoch: 3855.1947486400604 secs

INFO:training log:Epoch 24/30
INFO:training log:final weights of first 3 elements of batch: [0.04003308 0.04016164 0.0401517  0.039858   0.03982032 0.04012056
 0.04001608 0.0401536  0.04009531 0.04006968 0.04020304 0.039982
 0.03997464 0.03997644 0.04006518 0.03992695 0.03997634 0.03982837
 0.03976432 0.03996918 0.03980253 0.04006078 0.04003078 0.03990253
 0.0400569 ], [0.04001488 0.04000647 0.04000999 0.04002324 0.04002611 0.04000285
 0.04001189 0.03998702 0.04002032 0.03997226 0.03994708 0.03997405
 0.04002552 0.04002602 0.03999062 0.03999322 0.03994362 0.04002035
 0.04001671 0.04000536 0.04001025 0.03998882 0.04001253 0.03994667
 0.04002406], [0.04036171 0.03931246 0.04027119 0.03992652 0.04003363 0.04028276
 0.0397349  0.04019198 0.03975349 0.03992851 0.03968205 0.03966895
 0.04010399 0.04038497 0.0397008  0.03977391 0.04039029 0.03999993
 0.04014847 0.04003689 0.04051181 0.03972929 0.04006363 0.04008387
 0.03992404]
INFO:training log:train loss 23.997650146484375 -  train mse loss: 0.002895463490858674 - train loss std (mse): 0.0005006241844967008 - val loss: 24.007287979125977 - val mse loss: 0.0022148706484586 - val loss std (mse): 0.0003437015402596444
INFO:training log:Time taken for 1 epoch: 3849.9071459770203 secs

INFO:training log:Epoch 25/30
INFO:training log:final weights of first 3 elements of batch: [0.04022593 0.03991495 0.03998981 0.04002364 0.04000731 0.03987838
 0.04013584 0.03981489 0.04021886 0.04009504 0.04007785 0.03978422
 0.04010931 0.03996111 0.04009686 0.03974849 0.04011903 0.0398776
 0.03995647 0.03983447 0.03996032 0.03994425 0.03990175 0.0401798
 0.04014379], [0.03999544 0.04000628 0.04001356 0.04000998 0.04001598 0.03999117
 0.04000015 0.03998048 0.04001381 0.04000097 0.03999723 0.04001183
 0.03998651 0.04001119 0.04001565 0.0399982  0.03999744 0.04000986
 0.03998749 0.040015   0.03997926 0.04000471 0.04000604 0.03999875
 0.039953  ], [0.03988022 0.04038976 0.04042767 0.04004386 0.04073374 0.03988485
 0.04053611 0.0398856  0.03992419 0.03933461 0.03919042 0.03984454
 0.03995159 0.04016748 0.03954534 0.03926497 0.03986144 0.0404008
 0.03992037 0.03967198 0.04034024 0.03945598 0.04076751 0.04041941
 0.04015733]
INFO:training log:train loss 23.998212814331055 -  train mse loss: 0.00322498450987041 - train loss std (mse): 0.000553016783669591 - val loss: 24.01291275024414 - val mse loss: 0.0022497696336358786 - val loss std (mse): 0.000343350344337523
INFO:training log:Time taken for 1 epoch: 3852.5359647274017 secs

INFO:training log:Epoch 26/30
INFO:training log:final weights of first 3 elements of batch: [0.04015401 0.03992851 0.03972122 0.04001318 0.03980155 0.03999527
 0.04016338 0.03992813 0.04003779 0.04004655 0.03990895 0.04004407
 0.03994341 0.03997549 0.04002552 0.03998228 0.03997314 0.04006907
 0.04006896 0.04016239 0.04005706 0.04001711 0.03987583 0.04005888
 0.04004831], [0.04000731 0.04001117 0.039995   0.03998964 0.04001021 0.04000831
 0.04000555 0.04000851 0.03997341 0.04000928 0.04001109 0.0399953
 0.04001141 0.04001132 0.04000103 0.04000638 0.04001112 0.04000323
 0.03999206 0.04000536 0.03993156 0.04000923 0.04000766 0.03998472
 0.0400001 ], [0.0398882  0.03982469 0.04027044 0.03956516 0.03997097 0.04014376
 0.03999929 0.04048816 0.04015112 0.04037818 0.04015314 0.03988813
 0.04020917 0.03964279 0.04006384 0.04014052 0.04007911 0.03959555
 0.03977507 0.03952352 0.0401137  0.03989044 0.03976475 0.04052094
 0.0399593 ]
INFO:training log:train loss 23.9950008392334 -  train mse loss: 0.0030290293507277966 - train loss std (mse): 0.0005207210779190063 - val loss: 24.01560401916504 - val mse loss: 0.0021765960846096277 - val loss std (mse): 0.00033373499172739685
INFO:training log:Time taken for 1 epoch: 3840.2069823741913 secs

INFO:training log:Epoch 27/30
INFO:training log:final weights of first 3 elements of batch: [0.03998449 0.04003914 0.0398966  0.03998111 0.04001348 0.04014737
 0.03990228 0.04009181 0.03986617 0.04000812 0.04002627 0.03991751
 0.03997549 0.04012967 0.04007319 0.04001103 0.03988524 0.04000255
 0.04002804 0.04015189 0.04006199 0.04013525 0.03970223 0.03992694
 0.04004214], [0.03997424 0.04001398 0.04001161 0.0399905  0.04001211 0.03998528
 0.03998848 0.03997602 0.04001361 0.03999439 0.0399718  0.04000016
 0.04001313 0.0400096  0.04000871 0.04000557 0.04001355 0.03998743
 0.0400121  0.03999451 0.04001128 0.04000224 0.04000681 0.04000437
 0.03999849], [0.04051876 0.03990667 0.03988074 0.04010749 0.0395688  0.03965406
 0.03978346 0.03997945 0.04000745 0.03993874 0.04011466 0.04061295
 0.03990761 0.03984267 0.04000757 0.03986751 0.04003209 0.04006451
 0.03974995 0.03980902 0.04054393 0.0395722  0.03989172 0.04060729
 0.04003079]
INFO:training log:train loss 24.004560470581055 -  train mse loss: 0.0029309545643627644 - train loss std (mse): 0.0005015803617425263 - val loss: 24.006149291992188 - val mse loss: 0.0021282974630594254 - val loss std (mse): 0.0003247193235438317
INFO:training log:Time taken for 1 epoch: 3843.7682757377625 secs

INFO:training log:Epoch 28/30
INFO:training log:final weights of first 3 elements of batch: [0.03997023 0.04009812 0.03998767 0.040058   0.03981927 0.04003111
 0.04013904 0.04007964 0.03989984 0.0400225  0.03983989 0.040061
 0.039963   0.0400645  0.03991899 0.04009536 0.03990886 0.03997995
 0.04007564 0.03998034 0.03989821 0.03990462 0.04027069 0.03994451
 0.03998904], [0.04000924 0.03999858 0.04000248 0.04001416 0.04001416 0.04001046
 0.0400141  0.03998305 0.04001211 0.04000552 0.04000877 0.04001053
 0.04001378 0.0399682  0.03999177 0.03997115 0.04000224 0.04000222
 0.04000521 0.03998737 0.04001072 0.0400122  0.03997039 0.04000729
 0.03997426], [0.04013685 0.03997836 0.04080171 0.03994471 0.04015896 0.03993282
 0.03995702 0.03999516 0.0401169  0.04024143 0.0393011  0.03955372
 0.03984134 0.04029962 0.04033042 0.03968946 0.03997261 0.03999689
 0.03968681 0.0402865  0.03987916 0.03999798 0.04027625 0.03943893
 0.04018534]
INFO:training log:train loss 24.009611129760742 -  train mse loss: 0.002797950292006135 - train loss std (mse): 0.0004737233684863895 - val loss: 23.997465133666992 - val mse loss: 0.0021562925539910793 - val loss std (mse): 0.00032811780693009496
INFO:training log:Time taken for 1 epoch: 3842.4887869358063 secs

INFO:training log:Epoch 29/30
INFO:training log:final weights of first 3 elements of batch: [0.03997215 0.04004177 0.04005628 0.03990915 0.0399884  0.04002599
 0.04011822 0.03998512 0.03998578 0.04010661 0.03999195 0.04000365
 0.03984813 0.04007992 0.03984117 0.04010779 0.04007959 0.04001188
 0.04005666 0.03978528 0.03991463 0.04014098 0.03994103 0.03992998
 0.04007793], [0.03999224 0.04001723 0.03997684 0.04000576 0.03997342 0.03998689
 0.04000979 0.04000936 0.04001679 0.0399867  0.04001824 0.04001127
 0.03995973 0.04000313 0.04001051 0.04001711 0.03995886 0.03996895
 0.04001801 0.04000489 0.04001413 0.03999459 0.04001352 0.04001785
 0.04001419], [0.04016222 0.03993842 0.0394658  0.03963326 0.03988813 0.03961797
 0.03986729 0.03978977 0.0402681  0.0399211  0.04058912 0.03990509
 0.04011872 0.04002836 0.03996719 0.04016576 0.04005061 0.03950133
 0.04025818 0.03965704 0.04061687 0.0403818  0.03974093 0.03987932
 0.04058754]
INFO:training log:train loss 24.006973266601562 -  train mse loss: 0.002669744426384568 - train loss std (mse): 0.00046002757153473794 - val loss: 24.006391525268555 - val mse loss: 0.0020602657459676266 - val loss std (mse): 0.000317008321871981
INFO:training log:Time taken for 1 epoch: 3845.5010595321655 secs

INFO:training log:Epoch 30/30
INFO:training log:final weights of first 3 elements of batch: [0.03997446 0.04002723 0.0398929  0.04012544 0.04004497 0.03992148
 0.04005432 0.03995308 0.03999    0.04007926 0.03987624 0.03996873
 0.03996945 0.04004598 0.04002355 0.03994432 0.04009004 0.03990498
 0.03996668 0.04007586 0.0401567  0.0398952  0.04005299 0.04004454
 0.03992166], [0.03998434 0.03999186 0.03999852 0.04000925 0.04000868 0.03999773
 0.04002213 0.03997111 0.04000831 0.04001847 0.03998103 0.0400017
 0.04002223 0.04000649 0.03999549 0.03999758 0.03999409 0.03999245
 0.03998591 0.04001924 0.04001973 0.03998103 0.04001023 0.03996423
 0.04001817], [0.03965059 0.04012665 0.03992268 0.03986219 0.04014309 0.04000458
 0.04032343 0.03975271 0.03968234 0.03990887 0.04046433 0.03961566
 0.03965475 0.04025839 0.03985551 0.03987164 0.04035006 0.04001298
 0.04075476 0.03996236 0.03986483 0.04003435 0.04022163 0.03991248
 0.03978921]
INFO:training log:train loss 24.004594802856445 -  train mse loss: 0.002805662341415882 - train loss std (mse): 0.0004675577220041305 - val loss: 24.000513076782227 - val mse loss: 0.002090298570692539 - val loss std (mse): 0.000308712653350085
INFO:training log:Time taken for 1 epoch: 3853.8987901210785 secs

INFO:training log:total training time for 30 epochs:115416.15156960487
INFO:training log:saving loss and metrics information...
INFO:training log:saving model output in .npy files...
INFO:training log:training of SMC Transformer for a time-series dataset done...
INFO:training log:>>>--------------------------------------------------------------------------------------------------------------------------------------------------------------<<<
INFO:training log:skipping training...
INFO:training log:start evaluation...
INFO:training log:Latest checkpoint restored!!
INFO:training log:<------------------------computing latest statistics----------------------------------------------------------------------------------------->
WARNING:tensorflow:Layer smc__transformer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

INFO:training log:saving predictions on validation set...
INFO:training log:predictions shape: (40, 1048, 25, 24, 1)
INFO:training log:train mse loss:0.0031990972347557545 - train loss std (mse):0.0003302381082903594 - val mse loss:0.0021688148844987154 - val loss (mse) std: 0.00031414872501045465
INFO:training log:unistep evaluation with N=1...
INFO:training log:test mse loss: 0.0021754817571491003 - test loss std(mse) - 0.00031365937320515513
INFO:training log:saving predictions for test set in .npy files...
INFO:training log:predictions shape for test set:(41981, 25, 24, 1)
INFO:training log:unormalized predictions shape for test set:(41981, 25, 24, 1)
