INFO:training log:model hyperparameters from the config file: {'num_layers': 1, 'num_heads': 1, 'd_model': 3, 'dff': 12, 'rate': 0.1, 'maximum_position_encoding_baseline': 'None', 'maximum_position_encoding_smc': 'None'}
INFO:training log:smc hyperparameters from the config file: {'num_particles': 5, 'noise_encoder': 'False', 'noise_SMC_layer': 'True', 'sigma': 0.05}
INFO:training log:starting the training of the smc transformer...
INFO:training log:number of training samples: 336290
INFO:training log:steps per epoch: 320
WARNING:tensorflow:Layer smc__transformer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

INFO:training log:Epoch 1/30
INFO:training log:model hyperparameters from the config file: {'num_layers': 1, 'num_heads': 1, 'd_model': 3, 'dff': 12, 'rate': 0.1, 'maximum_position_encoding_baseline': 'None', 'maximum_position_encoding_smc': 'None'}
INFO:training log:smc hyperparameters from the config file: {'num_particles': 5, 'noise_encoder': 'False', 'noise_SMC_layer': 'True', 'sigma': 0.05}
INFO:training log:starting the training of the smc transformer...
INFO:training log:number of training samples: 336290
INFO:training log:steps per epoch: 320
WARNING:tensorflow:Layer smc__transformer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

INFO:training log:Epoch 1/30
INFO:training log:final weights of first 3 elements of batch: [0.19972926 0.19991873 0.20017174 0.2001516  0.2000287 ], [0.199961   0.19994949 0.19995502 0.19998375 0.20015076], [0.19987136 0.19991155 0.20003179 0.20018184 0.20000345]
INFO:training log:train loss 2.3051846027374268 -  train mse loss: 0.7990691661834717 - val loss: 2.1712992191314697 - val mse loss: 0.6730459928512573
INFO:training log:Time taken for 1 epoch: 750.6820726394653 secs

INFO:training log:Epoch 2/30
INFO:training log:final weights of first 3 elements of batch: [0.19999981 0.19999972 0.1999999  0.19999978 0.2000008 ], [0.19983493 0.19997104 0.20011777 0.20010038 0.19997591], [0.20005856 0.19994128 0.20002267 0.19994527 0.20003225]
INFO:training log:train loss 1.7940571308135986 -  train mse loss: 0.28690358996391296 - val loss: 1.6962924003601074 - val mse loss: 0.20096701383590698
INFO:training log:Time taken for 1 epoch: 752.208740234375 secs

INFO:training log:Epoch 3/30
INFO:training log:model hyperparameters from the config file: {'num_layers': 1, 'num_heads': 1, 'd_model': 3, 'dff': 12, 'rate': 0.1, 'maximum_position_encoding_baseline': 'None', 'maximum_position_encoding_smc': 'None'}
INFO:training log:smc hyperparameters from the config file: {'num_particles': 5, 'noise_encoder': 'False', 'noise_SMC_layer': 'True', 'sigma': 0.05}
INFO:training log:starting the training of the smc transformer...
INFO:training log:number of training samples: 336290
INFO:training log:steps per epoch: 320
INFO:training log:Latest checkpoint restored!!
WARNING:tensorflow:Layer smc__transformer is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.

If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.

To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.

INFO:training log:starting training after checkpoint restoring from epoch 2
INFO:training log:Epoch 3/30
INFO:training log:final weights of first 3 elements of batch: [0.20102309 0.20075373 0.19985922 0.19861275 0.19975126], [0.20013258 0.19995798 0.2000888  0.19992673 0.19989388], [0.19991116 0.19995217 0.20004883 0.19994846 0.20013951]
INFO:training log:train loss 1.6256636381149292 -  train mse loss: 0.11954814940690994 - val loss: 1.5725165605545044 - val mse loss: 0.07426328212022781
INFO:training log:Time taken for 1 epoch: 759.708479642868 secs

INFO:training log:Epoch 4/30
INFO:training log:final weights of first 3 elements of batch: [0.19896579 0.20066887 0.20111857 0.19875742 0.20048937], [0.1998262  0.20000614 0.20005836 0.2001211  0.1999882 ], [0.19971517 0.19996108 0.19982332 0.20017993 0.20032057]
INFO:training log:train loss 1.5268497467041016 -  train mse loss: 0.019696224480867386 - val loss: 1.5016347169876099 - val mse loss: 0.0063094464130699635
INFO:training log:Time taken for 1 epoch: 763.9330382347107 secs

INFO:training log:Epoch 5/30
INFO:training log:final weights of first 3 elements of batch: [0.2004167  0.19976974 0.19943656 0.2000181  0.2003589 ], [0.19974408 0.19993189 0.20038022 0.20029676 0.1996471 ], [0.19998531 0.19995664 0.19993998 0.20009154 0.20002657]
INFO:training log:train loss 1.503492832183838 -  train mse loss: 0.006313967518508434 - val loss: 1.5071247816085815 - val mse loss: 0.0037450080271810293
INFO:training log:Time taken for 1 epoch: 764.5905296802521 secs

INFO:training log:Epoch 6/30
INFO:training log:final weights of first 3 elements of batch: [0.20065476 0.19994651 0.20006481 0.1992383  0.20009564], [0.20003079 0.19999506 0.19988644 0.20002645 0.20006125], [0.19987637 0.19999976 0.19996221 0.20007811 0.20008346]
INFO:training log:train loss 1.5050262212753296 -  train mse loss: 0.004328395705670118 - val loss: 1.5018032789230347 - val mse loss: 0.002824418945237994
INFO:training log:Time taken for 1 epoch: 765.7989020347595 secs

INFO:training log:Epoch 7/30
INFO:training log:final weights of first 3 elements of batch: [0.20002003 0.20064804 0.20093548 0.19821362 0.20018287], [0.19994433 0.20018771 0.19983181 0.19995289 0.20008326], [0.19996904 0.20011935 0.19963291 0.20028934 0.19998932]
INFO:training log:train loss 1.5020216703414917 -  train mse loss: 0.0035553814377635717 - val loss: 1.5007444620132446 - val mse loss: 0.0026267319917678833
INFO:training log:Time taken for 1 epoch: 764.993762254715 secs

INFO:training log:Epoch 8/30
INFO:training log:final weights of first 3 elements of batch: [0.19910076 0.19921936 0.20120417 0.20084277 0.19963299], [0.20000376 0.19983838 0.19977939 0.20022367 0.20015484], [0.20021072 0.19967814 0.20000994 0.20021841 0.1998828 ]
INFO:training log:train loss 1.5008831024169922 -  train mse loss: 0.003912573680281639 - val loss: 1.5024728775024414 - val mse loss: 0.002606675261631608
INFO:training log:Time taken for 1 epoch: 762.4462890625 secs

INFO:training log:Epoch 9/30
INFO:training log:final weights of first 3 elements of batch: [0.1998574  0.20046899 0.20032984 0.19929972 0.20004398], [0.200056   0.20012856 0.19987454 0.19993626 0.20000468], [0.19961295 0.20006031 0.20029363 0.200064   0.19996914]
INFO:training log:train loss 1.5051891803741455 -  train mse loss: 0.004193514585494995 - val loss: 1.4989359378814697 - val mse loss: 0.002696071285754442
INFO:training log:Time taken for 1 epoch: 761.5263161659241 secs

INFO:training log:Epoch 10/30
INFO:training log:final weights of first 3 elements of batch: [0.20133463 0.19954644 0.19948335 0.20019017 0.1994454 ], [0.20009093 0.20011516 0.19983074 0.19982131 0.20014186], [0.19984162 0.2001285  0.19994278 0.20002042 0.20006675]
INFO:training log:train loss 1.5026969909667969 -  train mse loss: 0.00369707471691072 - val loss: 1.5034582614898682 - val mse loss: 0.002460325136780739
INFO:training log:Time taken for 1 epoch: 762.7314178943634 secs

INFO:training log:Epoch 11/30
INFO:training log:final weights of first 3 elements of batch: [0.20038204 0.20082605 0.19929403 0.1997856  0.19971225], [0.2000214  0.20002155 0.20009555 0.20001449 0.19984701], [0.2000261  0.2002169  0.19988897 0.19997497 0.1998931 ]
INFO:training log:train loss 1.501734733581543 -  train mse loss: 0.003912431187927723 - val loss: 1.5071909427642822 - val mse loss: 0.002595552010461688
INFO:training log:Time taken for 1 epoch: 763.3301720619202 secs

INFO:training log:Epoch 12/30
INFO:training log:final weights of first 3 elements of batch: [0.20070659 0.19929843 0.199383   0.19964604 0.200966  ], [0.20006149 0.1998943  0.1997576  0.20020476 0.2000818 ], [0.20007685 0.1999422  0.200225   0.20000637 0.19974953]
INFO:training log:train loss 1.5052597522735596 -  train mse loss: 0.003931512124836445 - val loss: 1.4997808933258057 - val mse loss: 0.0022992200683802366
INFO:training log:Time taken for 1 epoch: 762.7469823360443 secs

INFO:training log:Epoch 13/30
INFO:training log:final weights of first 3 elements of batch: [0.20011032 0.20021044 0.19992141 0.19972168 0.2000362 ], [0.1999424  0.20013258 0.20012704 0.1997788  0.2000192 ], [0.19996648 0.19993606 0.2000298  0.20032828 0.19973932]
INFO:training log:train loss 1.5083540678024292 -  train mse loss: 0.003926489036530256 - val loss: 1.499131441116333 - val mse loss: 0.0026924998965114355
INFO:training log:Time taken for 1 epoch: 764.1364934444427 secs

INFO:training log:Epoch 14/30
INFO:training log:final weights of first 3 elements of batch: [0.19899568 0.19970317 0.19947873 0.20228949 0.19953282], [0.20011078 0.20008232 0.19976379 0.19992417 0.20011891], [0.19969906 0.19997466 0.19989786 0.20033522 0.20009314]
INFO:training log:train loss 1.5073519945144653 -  train mse loss: 0.004741828888654709 - val loss: 1.502903938293457 - val mse loss: 0.0026203300803899765
INFO:training log:Time taken for 1 epoch: 762.7327065467834 secs

INFO:training log:Epoch 15/30
INFO:training log:final weights of first 3 elements of batch: [0.1988426  0.19986784 0.19987865 0.20108789 0.20032306], [0.1997327  0.19981849 0.2002458  0.20021452 0.1999884 ], [0.19999401 0.1997792  0.20007582 0.20015197 0.19999897]
INFO:training log:train loss 1.50706148147583 -  train mse loss: 0.003505710046738386 - val loss: 1.508677363395691 - val mse loss: 0.002309915143996477
INFO:training log:Time taken for 1 epoch: 763.3065633773804 secs

INFO:training log:Epoch 16/30
INFO:training log:final weights of first 3 elements of batch: [0.20019557 0.20064895 0.19974202 0.19936149 0.20005202], [0.20031989 0.19955154 0.20018464 0.19969209 0.20025177], [0.19996876 0.19977205 0.19999357 0.20011836 0.20014727]
INFO:training log:train loss 1.503079891204834 -  train mse loss: 0.00360751966945827 - val loss: 1.4996490478515625 - val mse loss: 0.0022811542730778456
INFO:training log:Time taken for 1 epoch: 762.718020439148 secs

INFO:training log:Epoch 17/30
INFO:training log:final weights of first 3 elements of batch: [0.19923729 0.20153828 0.19995566 0.19970886 0.19955994], [0.19982253 0.20012681 0.19993994 0.20015945 0.19995123], [0.19934909 0.19962914 0.19944124 0.20066349 0.20091704]
INFO:training log:train loss 1.5057505369186401 -  train mse loss: 0.0038579748943448067 - val loss: 1.4998283386230469 - val mse loss: 0.002236742526292801
INFO:training log:Time taken for 1 epoch: 763.9365532398224 secs

INFO:training log:Epoch 18/30
INFO:training log:final weights of first 3 elements of batch: [0.19912584 0.20186062 0.200793   0.19938578 0.19883482], [0.20002998 0.199677   0.20006895 0.20004597 0.20017813], [0.19935375 0.2001423  0.20045881 0.2003678  0.19967738]
INFO:training log:train loss 1.4987984895706177 -  train mse loss: 0.003629905404523015 - val loss: 1.502272367477417 - val mse loss: 0.0022715080995112658
INFO:training log:Time taken for 1 epoch: 762.6435086727142 secs

INFO:training log:Epoch 19/30
INFO:training log:final weights of first 3 elements of batch: [0.20001386 0.20123541 0.19902408 0.19893853 0.20078805], [0.20029648 0.19972143 0.20001939 0.19994639 0.20001632], [0.1999269  0.20001867 0.20006554 0.19992128 0.20006762]
INFO:training log:train loss 1.5014705657958984 -  train mse loss: 0.0035212645307183266 - val loss: 1.4990744590759277 - val mse loss: 0.002253907732665539
INFO:training log:Time taken for 1 epoch: 762.3338820934296 secs

INFO:training log:Epoch 20/30
INFO:training log:final weights of first 3 elements of batch: [0.1975864  0.19972096 0.20032847 0.20076023 0.20160395], [0.19973962 0.20033339 0.19970244 0.20020594 0.20001863], [0.19997606 0.2000398  0.20003028 0.19990788 0.20004596]
INFO:training log:train loss 1.5058948993682861 -  train mse loss: 0.003741571446880698 - val loss: 1.5069694519042969 - val mse loss: 0.002264390466734767
INFO:training log:Time taken for 1 epoch: 761.8800625801086 secs

INFO:training log:Epoch 21/30
INFO:training log:final weights of first 3 elements of batch: [0.19948687 0.19939275 0.20010042 0.2004457  0.20057419], [0.19996642 0.20014818 0.19992897 0.19992739 0.200029  ], [0.19963232 0.20017207 0.20046158 0.19990677 0.19982724]
INFO:training log:train loss 1.5033735036849976 -  train mse loss: 0.003924740012735128 - val loss: 1.501591682434082 - val mse loss: 0.002377710770815611
INFO:training log:Time taken for 1 epoch: 762.399337053299 secs

INFO:training log:Epoch 22/30
INFO:training log:final weights of first 3 elements of batch: [0.20078294 0.19839607 0.20198832 0.19920391 0.19962879], [0.19972944 0.20022161 0.20022024 0.19965257 0.20017606], [0.19995001 0.19997688 0.19972262 0.20031098 0.20003949]
INFO:training log:train loss 1.5072898864746094 -  train mse loss: 0.003705174196511507 - val loss: 1.5001319646835327 - val mse loss: 0.0022689313627779484
INFO:training log:Time taken for 1 epoch: 762.1383879184723 secs

INFO:training log:Epoch 23/30
INFO:training log:final weights of first 3 elements of batch: [0.19981319 0.19998813 0.20074128 0.20003735 0.19942006], [0.200241   0.19989918 0.19993575 0.19977492 0.20014924], [0.19978166 0.1999685  0.19976348 0.20038152 0.20010488]
INFO:training log:train loss 1.5013455152511597 -  train mse loss: 0.003513957606628537 - val loss: 1.4994597434997559 - val mse loss: 0.0022512455470860004
INFO:training log:Time taken for 1 epoch: 762.6093690395355 secs

INFO:training log:Epoch 24/30
INFO:training log:final weights of first 3 elements of batch: [0.20033227 0.1992993  0.19953054 0.20087408 0.19996372], [0.19990748 0.20003428 0.1997414  0.20012113 0.20019571], [0.19993447 0.20023859 0.20000003 0.2000202  0.19980672]
INFO:training log:train loss 1.5035008192062378 -  train mse loss: 0.0036432319320738316 - val loss: 1.4996590614318848 - val mse loss: 0.0023197380360215902
INFO:training log:Time taken for 1 epoch: 765.2696285247803 secs

INFO:training log:Epoch 25/30
INFO:training log:final weights of first 3 elements of batch: [0.19942392 0.19978978 0.20023623 0.20108747 0.19946258], [0.20000899 0.20004573 0.19988778 0.19996417 0.20009337], [0.19982174 0.20025416 0.20007853 0.19990838 0.19993722]
INFO:training log:train loss 1.4980188608169556 -  train mse loss: 0.0034990087151527405 - val loss: 1.5005850791931152 - val mse loss: 0.0022998498752713203
INFO:training log:Time taken for 1 epoch: 764.5136919021606 secs

INFO:training log:Epoch 26/30
INFO:training log:final weights of first 3 elements of batch: [0.19956675 0.20025118 0.19967043 0.20026769 0.20024404], [0.20007184 0.20003586 0.19997948 0.19993323 0.1999796 ], [0.20020619 0.19997375 0.1999242  0.19982448 0.20007141]
INFO:training log:train loss 1.5034546852111816 -  train mse loss: 0.00355868530459702 - val loss: 1.4971437454223633 - val mse loss: 0.00221065036021173
INFO:training log:Time taken for 1 epoch: 763.3883545398712 secs

INFO:training log:Epoch 27/30
INFO:training log:final weights of first 3 elements of batch: [0.1982097  0.19975623 0.20017013 0.2001997  0.20166425], [0.20002352 0.20004025 0.20006856 0.19992223 0.19994539], [0.20002133 0.20013788 0.19968583 0.20006202 0.20009297]
INFO:training log:train loss 1.4994032382965088 -  train mse loss: 0.0036173630505800247 - val loss: 1.4999362230300903 - val mse loss: 0.002170141087844968
INFO:training log:Time taken for 1 epoch: 764.8150146007538 secs

INFO:training log:Epoch 28/30
INFO:training log:final weights of first 3 elements of batch: [0.19891398 0.20051779 0.20020473 0.20033951 0.200024  ], [0.20005056 0.2002978  0.19999605 0.2001448  0.1995108 ], [0.19976242 0.19953379 0.20033287 0.2004502  0.19992071]
INFO:training log:train loss 1.4975570440292358 -  train mse loss: 0.003354633692651987 - val loss: 1.5076872110366821 - val mse loss: 0.002227585297077894
INFO:training log:Time taken for 1 epoch: 765.4747018814087 secs

INFO:training log:Epoch 29/30
INFO:training log:final weights of first 3 elements of batch: [0.19876194 0.20038204 0.19998388 0.20006096 0.20081116], [0.20004542 0.19991939 0.1999302  0.20023608 0.1998689 ], [0.19987452 0.20012894 0.19984137 0.2000924  0.20006284]
INFO:training log:train loss 1.5033214092254639 -  train mse loss: 0.0036550352815538645 - val loss: 1.504082441329956 - val mse loss: 0.002281417604535818
INFO:training log:Time taken for 1 epoch: 763.0163028240204 secs

INFO:training log:Epoch 30/30
INFO:training log:final weights of first 3 elements of batch: [0.19999398 0.19983822 0.20016769 0.19942613 0.20057395], [0.20006526 0.2000039  0.19968441 0.20011409 0.20013236], [0.20004031 0.19988593 0.20007296 0.19993411 0.2000666 ]
INFO:training log:train loss 1.4993565082550049 -  train mse loss: 0.003484770655632019 - val loss: 1.5028364658355713 - val mse loss: 0.0022833957336843014
INFO:training log:Time taken for 1 epoch: 762.292884349823 secs

INFO:training log:total training time for 30 epochs:21371.41980934143
INFO:training log:saving loss and metrics information...
